{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DATA_DIR = \"../data/social\"\n",
    "\n",
    "def data_file(name: str) -> str: return f\"{DATA_DIR}/{name}.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aggregation (TikTok)\n",
    "\n",
    "First we want to collect a set of TikTok posts to use for developing our social media post encoder. To do this, we will need to extract the `item_list` request that is used by the TikTok webapp to request video content during the infinite scroll on the For You Page. We can use inspect element to grab the URLs of these requests as we scroll on the For You Page. Then, by copying and pasting this URL into our python code, we can retrieve all the relevant metadata for the posts we are being fed by TikTok. \n",
    "\n",
    "This will provide us with plenty of video metadata, which we can then clean to only save the videos that have longer, more relevant metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "def extract_data(data: dict) -> dict:\n",
    "    extracted_data = {\n",
    "        \"id\": data['id'],\n",
    "        \"description\": data['desc'],\n",
    "        \"author\": {\n",
    "            \"id\": data['author']['id'],\n",
    "            \"name\": data['author']['nickname'],\n",
    "            \"signature\": data['author']['signature'],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "def save_results(results) -> None:\n",
    "    data = results['itemList']\n",
    "\n",
    "    with open(data_file(\"tik_tok\"), \"r\", encoding=\"utf-8\") as tik_tok:\n",
    "        current_data = json.load(tik_tok)\n",
    "\n",
    "    with open(data_file(\"tik_tok\"), \"w\", encoding=\"utf-8\") as tik_tok:\n",
    "        extracted_data = [extract_data(item) for item in data]\n",
    "        current_data.extend(extracted_data)\n",
    "        json.dump(current_data, tik_tok, indent=4)\n",
    "        print(f\"Added {len(extracted_data)} new results to tik_tok.json. Total results: {len(current_data)}\")\n",
    "\n",
    "\n",
    "def pipeline() -> None:\n",
    "    url = input(\"Enter the url: \")\n",
    "    print(\"Requesting data...\")\n",
    "    results = requests.get(url)\n",
    "\n",
    "    if results.status_code == 200:\n",
    "        save_results(results.json())\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: ({results.status_code}) {results.text})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results: 108\n",
      "Unique results: 107\n"
     ]
    }
   ],
   "source": [
    "# pipeline()1\n",
    "\n",
    "# Check for duplicates\n",
    "with open(data_file(\"tik_tok\"), \"r\", encoding=\"utf-8\") as tik_tok:\n",
    "    data = json.load(tik_tok)\n",
    "\n",
    "    ids = [item['id'] for item in data]\n",
    "    print(f\"Total results: {len(ids)}\")\n",
    "    print(f\"Unique results: {len(set(ids))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Now that we have the data aggregated through our pipeline, we want to create a cleaning pipeline to remove all the entries with short description and limited metadata. Let's begin by doing some EDA on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "def load_data() -> pd.DataFrame:\n",
    "    with open(data_file(\"tik_tok\"), \"r\", encoding=\"utf-8\") as tik_tok:\n",
    "        data = json.load(tik_tok)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def annotate_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "    df['description_length'] = df['description'].apply(lambda val: len(enc.encode(val)))\n",
    "    df['author_signature_length'] = df['author_signature'].apply(lambda val: len(enc.encode(val)))\n",
    "\n",
    "    return df\n",
    "\n",
    "def overwrite_results(results) -> None:\n",
    "    with open(data_file(\"tik_tok\"), \"w\", encoding=\"utf-8\") as tik_tok:\n",
    "        json.dump(results, tik_tok, indent=4)\n",
    "        print(f\"Overwrote results in tik_tok.json. Total results: {len(results)}\")\n",
    "\n",
    "def clean_pipeline():\n",
    "    data = load_data()\n",
    "    df = annotate_data(data)\n",
    "\n",
    "    start = len(df)\n",
    "\n",
    "    # Filter out all rows with description length < 30\n",
    "    df = df[df['description_length'] > 30]\n",
    "    # Filter out all rows with author signature length < 20\n",
    "    df = df[df['author_signature_length'] > 20]\n",
    "\n",
    "    end = len(df)\n",
    "\n",
    "    print(f\"Filtered out {start - end} rows.\")\n",
    "\n",
    "    # Convert the dataframe back into a JSON object\n",
    "    data = json.loads(df.to_json(orient=\"records\"))\n",
    "    overwrite_results(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keywords import KeywordVectorstore\n",
    "openai_embeddings = KeywordVectorstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "import torch  \n",
    "import torch.nn.functional as F\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from transformers import BertTokenizer,BertModel\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Does segmenting the input post result in better embeddings? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_query_term(row: pd.Series, segment=False):\n",
    "    if segment: \n",
    "        return f\"Author Name: {row['author_name']}. Post Description: {row['description']}. Author Description: {row['author_signature']}.\"\n",
    "    return f\"{row['author_name']} - {row['description']} - {row['author_signature']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms_default = df.apply(post_query_term, axis=1).tolist()\n",
    "search_terms_segmented = df.apply(post_query_term, args={\"segment\":True}, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.55"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns oai keywords for post, oai keywords for segmented post, intersection between the two\n",
    "def compare_segmented_input(idx):\n",
    "\n",
    "    post, segmented_post = search_terms_default[idx], search_terms_segmented[idx]\n",
    "    post_keywords, segmented_post_keywords = openai_embeddings.get_keywords([post, segmented_post])\n",
    "    post_keywords, segmented_post_keywords = set([p[0] for p in post_keywords]), set([p[0] for p in segmented_post_keywords])\n",
    "    return post_keywords, segmented_post_keywords, post_keywords.intersection(segmented_post_keywords)\n",
    "\n",
    "\n",
    "#calculate the average overlap between keywords for normal post and segmented post. \n",
    "def test_compare_segmented_input(num_cases):\n",
    "    overlaps = []\n",
    "    for idx in range(num_cases): \n",
    "        _,_, overlap = compare_segmented_input(idx)\n",
    "        overlaps.append(len(overlap))\n",
    "    return np.mean(overlaps)\n",
    "\n",
    "# test_compare_segmented_input(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "* over 20 cases, an average of 2.6/3 keywords found for segmented post are also found for non segmented post.  \n",
    "* segmenting the posts does not lead to different classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Does BERT provide comparable embeddings? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings: \n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n",
    "        self.model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.vs = self.build_vector_store()\n",
    "    \n",
    "    def embed(self, s, normalize_embedding = False, return_list = True):\n",
    "        inputs = self.tokenizer(s, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        with torch.no_grad(): \n",
    "            output = self.model(**inputs)\n",
    "        pool_embedding, last_hidden = output.pooler_output, output.last_hidden_state\n",
    "\n",
    "        if return_list: \n",
    "            pool_embedding = pool_embedding.squeeze(0).tolist()\n",
    "            # if normalize_embedding: \n",
    "            #     pool_embedding = self.normalize(pool_embedding)\n",
    "    \n",
    "        return pool_embedding\n",
    "\n",
    "    def build_vector_store(self, as_df = True):\n",
    "        # Creating embeddings for all keywords in the keywords list.\n",
    "\n",
    "        with open('../data/keywords.json', 'r') as file: \n",
    "            keywords = json.load(file)\n",
    "\n",
    "        keywords = reduce(lambda l1, l2: l1 + l2, keywords.values())\n",
    "        vs = {i: {'text': v, 'embedding': self.embed(v)} for i, v in enumerate(keywords)}\n",
    "        \n",
    "        if as_df: \n",
    "            vs = pd.DataFrame.from_dict(vs, orient='index')\n",
    "\n",
    "        return vs \n",
    "    \n",
    "    def search_vector_store(self, sentence, k = 5):\n",
    "        sent_embedding = self.embed(sentence)\n",
    "        sent_embedding_normalized = norm(sent_embedding)\n",
    "        vs = self.vs \n",
    "        vs[\"score\"] = vs.embedding.apply(lambda x: np.dot(sent_embedding,x)/(sent_embedding_normalized*norm(x)))\n",
    "  \n",
    "        vs.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "        top_keywords = [\n",
    "            (row[\"text\"], row[\"score\"])\n",
    "            for index, row in vs[[\"text\", \"score\"]].head(k).iterrows()\n",
    "        ]\n",
    "        return top_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_embeddings = BertEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare openai embeddings with bert embeddings\n",
    "def compare_oai_bert(post): \n",
    "    oai = openai_embeddings.get_keywords([post])[0]\n",
    "    bert = bert_embeddings.search_vector_store(post)\n",
    "    oai = set([pred[0] for pred in oai])\n",
    "    bert = set([pred[0] for pred in bert])\n",
    "\n",
    "    return oai, bert, oai.intersection(bert) \n",
    "\n",
    "def test_compare_oai_bert(num_cases):\n",
    "    overlaps = []\n",
    "    for idx in range(num_cases): \n",
    "        _, _, overlap = compare_oai_bert(search_terms_default[idx])\n",
    "        overlaps.append(len(overlap))\n",
    "    return np.mean(overlaps)\n",
    "\n",
    "# test_compare_oai_bert(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "* On average, 0 of 3 classes found by openAI embeddings are found by BERT embeddings. \n",
    "* The openAI embeddings out perform BERT embeddings, based on anecdotal comparisons of resultant classifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Does HuggingFace multi-label classification work out of the box, or does it require fine-tuning? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier: \n",
    "    def __init__(self) -> None:\n",
    "        bert_ckpt = \"distilbert-base-uncased\" \n",
    "        self.keywords = self.get_keywords()\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_ckpt)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            bert_ckpt,\n",
    "            num_labels=len(self.keywords)  ,\n",
    "            problem_type=\"multi_label_classification\",  \n",
    "        )\n",
    "        self.model.config.id2label =  self.keywords\n",
    "\n",
    "    def get_keywords(self):\n",
    "        with open('../data/keywords.json', 'r') as file:\n",
    "            keywords = json.load(file)\n",
    "        keywords = reduce(lambda l1, l2: l1 + l2, keywords.values())\n",
    "        keywords = {i: v for i, v in enumerate(keywords)}\n",
    "        return keywords \n",
    "    \n",
    "    def finetune(self, data):\n",
    "        # TODO: fine tune the classifier with some data \n",
    "        pass \n",
    "    \n",
    "    def classify(self, sequence, threshold=0.53):\n",
    "        inputs = self.tokenizer(sequence, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "        probabilities = F.sigmoid(logits)\n",
    "\n",
    "        label_indices = (probabilities > threshold).nonzero(as_tuple=True)[1]\n",
    "        assigned_labels = [label_indices[i].item() for i in range(len(label_indices))]\n",
    "        label_ids = [self.model.config.id2label[idx] for idx in assigned_labels]\n",
    "\n",
    "        return probabilities, label_ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_classifier = BertClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Post:\n",
      "<START> \n",
      "onzpo - Replying to @patrick_sea567 Nah, that's CAP. It's like I always say, I #onzpo use #vfx to make it look like I'm using a #noeyefilter but no #ihaveeyes check the  #lore lol - I'm glad you're still around!\n",
      "Smash Follow!üïπÔ∏è\n",
      "Business:chris@clickstalent.com \n",
      "<END>, \n",
      "Method 1) OpenAI embedding similarity: , \n",
      "Method 2) BERT multihead classification: ['Artisanal', 'Live Music', 'Exploration', 'Fitness Challenge'], \n",
      "          BERT classification scores: tensor([[0.5277, 0.5781, 0.4931, 0.5230, 0.5258, 0.4869, 0.4891, 0.5119, 0.4926,\n",
      "         0.4927, 0.5279, 0.4953, 0.4789, 0.4848, 0.5216, 0.4852, 0.4677, 0.4983,\n",
      "         0.5171, 0.5283, 0.5042, 0.5502, 0.5119, 0.4958, 0.4993, 0.4931, 0.5232,\n",
      "         0.5018, 0.4678, 0.5101, 0.5196, 0.4828, 0.4945, 0.4950, 0.4952, 0.5045,\n",
      "         0.4810, 0.4678, 0.5083, 0.4845, 0.5541, 0.4929, 0.5120, 0.4562, 0.4727,\n",
      "         0.5134, 0.4990, 0.5245, 0.4586, 0.4996, 0.5507, 0.5054, 0.5133, 0.4752,\n",
      "         0.4887, 0.4950, 0.5140, 0.4981, 0.4792, 0.5086]]), \n",
      "          \n",
      "          \n",
      "\n",
      "Input Post:\n",
      "<START> \n",
      "Darryl - Epic Street Interview Fail üôÜüèæ‚Äç‚ôÇÔ∏èüôÖüèæ‚Äç‚ôÇÔ∏è @Ow√© Collections #streetinterview #streetinterviews #streetprank #prank  - Street Interview - Saturdays @11am EST\n",
      "üìß: info@darrylskits.com\n",
      "üíå: $darrylskits \n",
      "<END>, \n",
      "Method 1) OpenAI embedding similarity: , \n",
      "Method 2) BERT multihead classification: ['Artisanal', 'Exotic Flavors', 'Sweet Treats', 'Live Music', 'Exploration', 'Fitness Challenge'], \n",
      "          BERT classification scores: tensor([[0.5224, 0.5700, 0.5010, 0.5349, 0.5148, 0.4956, 0.4947, 0.5173, 0.4936,\n",
      "         0.4895, 0.5314, 0.4933, 0.4827, 0.4854, 0.5223, 0.4871, 0.4811, 0.4956,\n",
      "         0.5157, 0.5164, 0.5105, 0.5430, 0.5162, 0.4930, 0.5099, 0.4931, 0.5219,\n",
      "         0.4973, 0.4795, 0.5105, 0.5114, 0.4915, 0.4801, 0.4965, 0.4922, 0.5103,\n",
      "         0.4945, 0.4768, 0.5053, 0.5001, 0.5422, 0.4822, 0.5097, 0.4584, 0.4710,\n",
      "         0.5132, 0.5089, 0.5217, 0.4588, 0.4989, 0.5469, 0.5019, 0.5110, 0.4690,\n",
      "         0.4789, 0.4810, 0.5141, 0.4956, 0.4917, 0.5050]]), \n",
      "          \n",
      "          \n"
     ]
    }
   ],
   "source": [
    "def evaluate_methods(post):\n",
    "    bert_probabilities, bert_categories = bert_classifier.classify(post)\n",
    "    relevant_keywords = '' #vs.get_keywords([post])\n",
    "\n",
    "    print()\n",
    "    print(f'''Input Post:\\n<START> \\n{post} \\n<END>, \n",
    "Method 1) OpenAI embedding similarity: {relevant_keywords}, \n",
    "Method 2) BERT multihead classification: {bert_categories}, \n",
    "          BERT classification scores: {bert_probabilities}, \n",
    "          \n",
    "          ''')\n",
    "# evaluate_methods(search_terms_default[0])\n",
    "# evaluate_methods(search_terms_default[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "* The classifier requires fine-tuning, since it does not automatically embed the input classes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
