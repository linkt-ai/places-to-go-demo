{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aggregation\n",
    "As a first step in the Places to Go Demo, we will need static venue data to create reccomendations from. In production, our venue sources will be managed by a Web Scraper bot that will handle crawling social media and updating the list based on activity. For now, we will populate a static set of 2,500 locations, which will be sourced from 5 cities. \n",
    "\n",
    "The five cities that have been requested by the client for the demo are:\n",
    "- **New York**\n",
    "- **Scottsdale**\n",
    "- **Miami**\n",
    "- **Los Angeles**\n",
    "- **Chicago**\n",
    "\n",
    "We will use the Yelp API to gather the top 500 rated locations in each city. We will then feed the `name` and `categories` field of each response to the AI model, which will seek to associate each venue with a list of keywords.\n",
    "\n",
    "We will need to take the following steps to achieve our task:\n",
    "1. Gather JSON objects for top 500 locations in each city\n",
    "2. Extract exhaustive list of all categories from the 2,500 locations\n",
    "3. Provide list of ChatGPT and prompt it to create a list of 20 keywords for each archetype\n",
    "4. Design prompt for associating businesses with keywords based on `name` and `categories` field \n",
    "5. Run list of 2,500 businesses and store results in a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "YELP_API_KEY = os.getenv(\"YELP_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gather JSON Data of Locations\n",
    "We want to start by using the `/businesses/search` endpoint of the Yelp Fusion API to gather the top 500 rated locations in each of our 5 cities. We will store these responses directly in JSON files to retrieve for future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# List of cities to search\n",
    "CITIES = [\"New%20York%20City\", \"Scottsdale\", \"Miami\", \"Los%20Angeles\", \"Chicago\"]\n",
    "CITY_CODES = [\"NYC\", \"SCOTTSDALE\", \"MIAMI\", \"LA\", \"CHICAGO\"]\n",
    "\n",
    "CITY_TO_CODE = dict(zip(CITIES, CITY_CODES))\n",
    "\n",
    "# Yelp Fusion API URL\n",
    "API_URL = \"https://api.yelp.com/v3\"\n",
    "BUSINESS_SEARCH_ENDPOINT = \"/businesses/search\"\n",
    "\n",
    "# Search Params For API Request\n",
    "LIMIT = 50\n",
    "SORT_BY = \"rating\"\n",
    "\n",
    "# Authorization\n",
    "HEADERS = {\n",
    "    \"Authorization\": \"Bearer \" + YELP_API_KEY,\n",
    "}\n",
    "\n",
    "def request_city_data(city: str):\n",
    "    \"\"\"Request data from Yelp API for a given city\"\"\"\n",
    "    base_url = f\"{API_URL}{BUSINESS_SEARCH_ENDPOINT}?location={city}&limit={LIMIT}&sort_by={SORT_BY}\"\n",
    "    url = base_url + \"&offset={}\"\n",
    "    offset = 0\n",
    "    data = []\n",
    "    for i in range(10):\n",
    "        results = requests.get(url.format(offset), headers=HEADERS).json()\n",
    "        \n",
    "        # Add the city code to the data\n",
    "        for result in results['businesses']: result['city'] = city\n",
    "\n",
    "        offset += LIMIT\n",
    "        data.extend(results[\"businesses\"])\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_location_data():\n",
    "    \"\"\"Extract location data from Yelp API\"\"\"\n",
    "    data = []\n",
    "    for city in tqdm(CITIES):\n",
    "        try:\n",
    "            print(\"Requesting data for\", city)\n",
    "            city_results = request_city_data(city)\n",
    "            data.extend(city_results)\n",
    "            print(f\"Received {len(city_results)} results for {city}. Total: {len(data)}\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to request data for\", city)\n",
    "            print(e)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting data for New%20York%20City\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:06<00:25,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received 500 results for New%20York%20City. Total: 500\n",
      "Requesting data for Scottsdale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:13<00:20,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received 500 results for Scottsdale. Total: 1000\n",
      "Requesting data for Miami\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:22<00:15,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received 500 results for Miami. Total: 1500\n",
      "Requesting data for Los%20Angeles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:30<00:08,  8.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received 500 results for Los%20Angeles. Total: 2000\n",
      "Requesting data for Chicago\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:37<00:00,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received 500 results for Chicago. Total: 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = extract_location_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/raw_location_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for locaction in data:\n",
    "        locaction['city_code'] = CITY_TO_CODE[locaction['city']]\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Exhuastive Category List from Locations\n",
    "We now want to get an exhaustive list of all the categories provided in our 2,500 locaitons. To do this, we will extract the `categories` field from each location, and append the values to a list. Once all values have been appended, we will type cast the list to a set to remove duplicates. \n",
    "\n",
    "We will store the category list in a JSON file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "for location in data:\n",
    "    loc_categories = [category['alias'] for category in location['categories']]\n",
    "    categories.extend(loc_categories)\n",
    "categories = list(set(categories))\n",
    "\n",
    "\n",
    "with open(\"../data/raw_categories.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(categories, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keyword List Generation\n",
    "We have used ChatGPT to convert our categories into keywords to use for classification. The keywords can be found in `../data/keywords.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt Engineering\n",
    "Now that we have our keywords set, we need to do some prompt engineering to create a GPT-3.5-Turbo prompt which associates a venue with a set of our keywords. To do this, there are a few considerations we must make:\n",
    "- Prompt must provide the list of keywords to the model\n",
    "- Model must accurately associate keywords with venues according to product needs\n",
    "- Want to process as many venues in one prompt as possible\n",
    "\n",
    "To provide the LLM with the list of keywords, we will simply provide them in the system prompt. For token efficiency, we may try to cram as many venues as possible into every prompt, so we limit the number of times we have to send a system prompt.\n",
    "\n",
    "In order to get accurate results without fine-tuning, we should take a few-shot approach, to do this, we will use ChatGPT to do ~20 locations, and we will then use these as an example for each prompt we send.\n",
    "\n",
    "Finally, we should try to jam as many tokens as possible into each prompt. We have 16k tokens to work with as a context window. We can use the examples to determine the optimal number of locations to use per prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    keywords = json.load(f)\n",
    "    raw_keywords = []\n",
    "    for cat, keyword in keywords.items():\n",
    "        words = [word.replace(\"-\", \" \") for word in keyword]\n",
    "        words = [word.title().replace(\" \", \"\") for word in words]\n",
    "        raw_keywords.extend(words)\n",
    "    keyword_set = set(raw_keywords)\n",
    "    \n",
    "\n",
    "with open(\"../data/example.json\", 'r', encoding='utf-8') as f:\n",
    "    example = json.load(f)\n",
    "\n",
    "def check_outputs(outputs):\n",
    "    output_keywords = []\n",
    "    for output in outputs['venues']:\n",
    "        output_keywords.extend(output['keywords'])\n",
    "\n",
    "    keyword_set = set(output_keywords)\n",
    "    # Ensure that the keywords are all contained in the raw keywords\n",
    "    if not keyword_set.issubset(raw_keywords):\n",
    "        raise ValueError(\"output_keywords is not a subset of raw_keywords\")\n",
    "\n",
    "check_outputs(example['output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    keywords = json.load(f)\n",
    "    keyword_list = []\n",
    "    for cat, keyword in keywords.items():\n",
    "        words = [word.replace(\"-\", \" \") for word in keyword]\n",
    "        words = [word.title().replace(\" \", \"\") for word in words]\n",
    "        keywords[cat] = words\n",
    "        keyword_list.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/location_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    location_data = json.load(f)\n",
    "    trimmed_location_data = []\n",
    "    for location in location_data:\n",
    "        trimmed_location_data.append({\n",
    "            \"id\": location['id'],\n",
    "            \"name\": location['name'],\n",
    "            \"city\": location['city_code'],\n",
    "            'rating': location['rating'],\n",
    "            \"categories\": [category['title'] for category in location['categories']]\n",
    "        })\n",
    "\n",
    "# Add an embed term to each location\n",
    "for location in trimmed_location_data:\n",
    "    categories = location['categories']\n",
    "    term = f\"{location['name']}: {', '.join(categories)}\"\n",
    "    location['embed_term'] = term\n",
    "    del location['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "vector_data = []\n",
    "\n",
    "def embed_terms(terms: List[str]):\n",
    "    response = client.embeddings.create(\n",
    "        input=terms,\n",
    "        model='text-embedding-ada-002'\n",
    "    )\n",
    "    return [datum.embedding for datum in response.data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_embeddings = embed_terms(raw_keywords)\n",
    "for keyword, embedding in zip(raw_keywords, keyword_embeddings):\n",
    "    vector_data.append({\n",
    "        \"keyword\": keyword,\n",
    "        \"vector\": embedding\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 2)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "for vector in vector_data:\n",
    "    vector['vector'] = np.array(vector['vector'])\n",
    "\n",
    "df = pd.DataFrame(vector_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Now, we want to create an embedding for each venue, by embedding the keywords\n",
    "terms = [location['embed_term'] for location in trimmed_location_data]\n",
    "embeddings = [*embed_terms(terms[:1000]), *embed_terms(terms[1000:2000]), *embed_terms(terms[2000:])]\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'venue': {'id': 'LslssTS75mcFf-6pttxKBQ',\n",
       "  'name': 'Panetino Bakery',\n",
       "  'city': 'NYC',\n",
       "  'rating': 5.0},\n",
       " 'keywords': ['Coffee Culture', 'Baking Passion', 'Gourmet']}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cypher_entities = []\n",
    "for i, data in enumerate(zip(embeddings, trimmed_location_data)):\n",
    "    embed, location = data\n",
    "    embed = np.array(embed)\n",
    "    vector_store = df.copy()\n",
    "    vector_store['similarity'] = df['vector'].apply(lambda x: np.dot(x, embed))\n",
    "    vector_store = vector_store.sort_values(by='similarity', ascending=False)\n",
    "    results = vector_store.keyword.values[:3].tolist()\n",
    "\n",
    "    cypher_entities.append({\n",
    "        'venue': {\n",
    "            'id': location['id'],\n",
    "            'name': location['name'],\n",
    "            'city': location['city'],\n",
    "            'rating': location['rating'],\n",
    "        },\n",
    "        'keywords': results,\n",
    "    })\n",
    "cypher_entities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/cypher_entites.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cypher_entities, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
