{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aggregation\n",
    "As a first step in the Places to Go Demo, we will need static venue data to create reccomendations from. In production, our venue sources will be managed by a Web Scraper bot that will handle crawling social media and updating the list based on activity. For now, we will populate a static set of 2,500 locations, which will be sourced from 5 cities. \n",
    "\n",
    "The five cities that have been requested by the client for the demo are:\n",
    "- **New York**\n",
    "- **Scottsdale**\n",
    "- **Miami**\n",
    "- **Los Angeles**\n",
    "- **Chicago**\n",
    "\n",
    "We will use the Yelp API to gather the top 500 rated locations in each city. We will then feed the `name` and `categories` field of each response to the AI model, which will seek to associate each venue with a list of keywords.\n",
    "\n",
    "We will need to take the following steps to achieve our task:\n",
    "1. Gather JSON objects for top 500 locations in each city\n",
    "2. Extract exhaustive list of all categories from the 2,500 locations\n",
    "3. Provide list of ChatGPT and prompt it to create a list of 20 keywords for each archetype\n",
    "4. Design prompt for associating businesses with keywords based on `name` and `categories` field \n",
    "5. Run list of 2,500 businesses and store results in a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "YELP_API_KEY = os.getenv(\"YELP_API_KEY\")\n",
    "TRIP_ADVISOR_API_KEY = os.getenv(\"TRIP_ADVISOR_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gather JSON Data of Locations\n",
    "We want to start by using the `/businesses/search` endpoint of the Yelp Fusion API to gather the top 500 rated locations in each of our 5 cities. We will store these responses directly in JSON files to retrieve for future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_TERMS = [\"tour\", \"activity\", \"experience\", \"resturant\", \"bar\", \"nightclub\", \"explore\", \"adventure\", \"museum\", \"nature\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "\n",
    "# List of cities to search\n",
    "CITIES = [\"New%20York%20City\", \"Scottsdale\", \"Miami\", \"Los%20Angeles\", \"Chicago\"]\n",
    "CITY_CODES = [\"NYC\", \"SCOTTSDALE\", \"MIAMI\", \"LA\", \"CHICAGO\"]\n",
    "\n",
    "CITY_TO_CODE = dict(zip(CITIES, CITY_CODES))\n",
    "\n",
    "# Yelp Fusion API URL\n",
    "API_URL = \"https://api.yelp.com/v3\"\n",
    "BUSINESS_SEARCH_ENDPOINT = \"/businesses/search\"\n",
    "\n",
    "# Search Params For API Request\n",
    "LIMIT = 50\n",
    "SORT_BY = \"best_match\"\n",
    "LOCALE = \"en_US\"\n",
    "\n",
    "# Authorization\n",
    "HEADERS = {\n",
    "    \"Authorization\": \"Bearer \" + YELP_API_KEY,\n",
    "}\n",
    "\n",
    "def request_city_data(city: str):\n",
    "    \"\"\"Request data from Yelp API for a given city\"\"\"\n",
    "    base_url = f\"{API_URL}{BUSINESS_SEARCH_ENDPOINT}?location={city}&limit={LIMIT}&sort_by={SORT_BY}&local={LOCALE}\"\n",
    "    data = []\n",
    "    for i, search_term in enumerate(SEARCH_TERMS):\n",
    "        url = base_url + f\"&term={search_term}\"\n",
    "        results = requests.get(url, headers=HEADERS).json()\n",
    "        # Add the city code to the data\n",
    "        for result in results['businesses']: result['city'] = city\n",
    "        data.extend(results[\"businesses\"])\n",
    "\n",
    "        # Reset the cursor to not interrupt the tqdm progress bar\n",
    "        print(f\"Found {len(results['businesses'])} results for {city} with term {search_term}\")\n",
    "    \n",
    "    # Log City Results\n",
    "    print(f\"Found {len(data)} results for {city}\")\n",
    "    return data\n",
    "\n",
    "def aggregate_city_data():\n",
    "    \"\"\"Aggregate data from all cities\"\"\"\n",
    "    data = []\n",
    "    for city in tqdm(CITIES):\n",
    "        data.extend(request_city_data(city))\n",
    "\n",
    "    print(\"\\r\", end=\"\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activity_data = aggregate_city_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# APPEND New Locations to Location Data -- DANGEROUS\n",
    "\n",
    "# with open(\"../data/searched_location_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     location_data = json.load(f)\n",
    "\n",
    "# total_locations = activity_data + location_data\n",
    "# total_location_ids = list(set([location['id'] for location in total_locations]))\n",
    "\n",
    "# locations = []\n",
    "# for _id in total_location_ids:\n",
    "#     for location in total_locations:\n",
    "#         if location['id'] == _id:\n",
    "#             locations.append(location)\n",
    "#             break\n",
    "\n",
    "# with open(\"../data/searched_location_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for loc in locations:\n",
    "#         loc['city_code'] = CITY_TO_CODE[loc['city']]\n",
    "#     json.dump(locations, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Web Scraping Script\n",
    "Run the webscraping script to get the reviews for each business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt Engineering\n",
    "Now that we have our keywords set, we need to do some prompt engineering to create a GPT-3.5-Turbo prompt which associates a venue with a set of our keywords. To do this, there are a few considerations we must make:\n",
    "- Prompt must provide the list of keywords to the model\n",
    "- Model must accurately associate keywords with venues according to product needs\n",
    "- Want to process as many venues in one prompt as possible\n",
    "\n",
    "To provide the LLM with the list of keywords, we will simply provide them in the system prompt. For token efficiency, we may try to cram as many venues as possible into every prompt, so we limit the number of times we have to send a system prompt.\n",
    "\n",
    "In order to get accurate results without fine-tuning, we should take a few-shot approach, to do this, we will use ChatGPT to do ~20 locations, and we will then use these as an example for each prompt we send.\n",
    "\n",
    "Finally, we should try to jam as many tokens as possible into each prompt. We have 16k tokens to work with as a context window. We can use the examples to determine the optimal number of locations to use per prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../scrape/locations_finished.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    location_data = json.load(f)\n",
    "    trimmed_location_data = []\n",
    "    for location in location_data:\n",
    "        trimmed_location_data.append({\n",
    "            \"id\": location['id'],\n",
    "            \"name\": location['name'],\n",
    "            \"city\": location['city_code'],\n",
    "            'rating': location['rating'],\n",
    "            \"reviews\": location['reviews'],\n",
    "            \"url\": location['url'],\n",
    "        })\n",
    "    location_data = trimmed_location_data\n",
    "    del trimmed_location_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add an embed term to each location\n",
    "for loc in location_data:\n",
    "    reviews = '\\n'.join([f\"{list(review.keys())[0]}:\\n{list(review.values())[0]}\" for review in loc['reviews']])\n",
    "    term = f\"{loc['name']}\\n\\n{reviews}\"\n",
    "    loc['embed_term'] = term\n",
    "\n",
    "embed_terms = [location['embed_term'] for location in location_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(embed_terms) == len(location_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "from typing import List\n",
    "\n",
    "categories = [\"Restaurant\", \"Activity\", \"Museum\", \"Outdoor Exploration\", \"Shopping\", \"Nightlife\", \"Historical Site\", \"Amusement Park\", \"Experience\", \"Relaxation\"]\n",
    "\n",
    "\n",
    "class CategoryVectorstore():\n",
    "\n",
    "    def __init__(self):\n",
    "        embeddings = self._embed(categories)\n",
    "        data = [{\"category\": category, \"embedding\": embedding} for category, embedding in zip(categories, embeddings)]\n",
    "        self.vectorstore = pd.DataFrame(data)\n",
    "\n",
    "    def _embed(self, terms: List[str]) -> List[List[float]]:\n",
    "        response = OpenAI().embeddings.create(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            input=terms,\n",
    "        )\n",
    "        return [result.embedding for result in response.data]\n",
    "\n",
    "    def _search(self, embedding: List[float]) -> str:\n",
    "        \"\"\"Search for the closest category to a given embedding\"\"\"\n",
    "        vectorstore = self.vectorstore.copy()\n",
    "        vectorstore['score'] = vectorstore.embedding.apply(lambda x: np.dot(x, embedding))\n",
    "        vectorstore.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "\n",
    "        category = vectorstore.iloc[0].category\n",
    "        return category\n",
    "\n",
    "    def get_categories(self, terms: List[str]) -> List[str]:\n",
    "        \"\"\"Get the categories for a list of terms\"\"\"\n",
    "        embeddings = self._embed(terms)\n",
    "        return [self._search(embedding) for embedding in embeddings]\n",
    "\n",
    "category_vectorstore = CategoryVectorstore()\n",
    "categories = [*category_vectorstore.get_categories(embed_terms[:1000]), *category_vectorstore.get_categories(embed_terms[1000:2000]), *category_vectorstore.get_categories(embed_terms[2000:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keywords import KeywordVectorstore\n",
    "\n",
    "vs = KeywordVectorstore()\n",
    "\n",
    "relevant_keywords = [*vs.get_keywords(embed_terms[:1000]), *vs.get_keywords(embed_terms[1000:2000]), *vs.get_keywords(embed_terms[2000:])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "location_and_data = zip(relevant_keywords, categories, location_data)\n",
    "\n",
    "cypher_entities = []\n",
    "for i, data in enumerate(location_and_data):\n",
    "    kewords, category, location = data\n",
    "    cypher_entities.append({\n",
    "        'venue': {\n",
    "            'id': location['id'],\n",
    "            'name': location['name'],\n",
    "            'city': location['city'],\n",
    "            'category': category,\n",
    "            'rating': location['rating'],\n",
    "            'url': urllib.parse.quote(location['url'], safe=''),\n",
    "        },\n",
    "        'reviews': location['reviews'],\n",
    "        'keywords': kewords,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'venue': {'id': 'RDKJqWixvHQ5XUYqovOWYw',\n",
       "  'name': 'Hammocks Trails',\n",
       "  'city': 'MIAMI',\n",
       "  'category': 'Outdoor Exploration',\n",
       "  'rating': 5.0,\n",
       "  'url': 'https%3A%2F%2Fwww.yelp.com%2Fbiz%2Fhammocks-trails-miami%3Fadjust_creative%3DFj58EcsIOfJVBvvlE6xZdw%26utm_campaign%3Dyelp_api_v3%26utm_medium%3Dapi_v3_business_search%26utm_source%3DFj58EcsIOfJVBvvlE6xZdw'},\n",
       " 'reviews': [{'Esmeralda Z': \"I come here with a group of girls to run this on Saturday mornings around 7am. It's super scenic and quiet here, perfect for jogging, walking and biking. There's a lot to look at while you put in those miles! We usually don't run the whole entire thing because we run a set amount of miles and turn around and run back. ( Since we park our cars at the small hammocks park.) So I'm not really sure if it connects anywhere...If it starts raining you can hide under the bridges or some small shelters they have available. Overall beautiful place to go run.\"},\n",
       "  {'Kareena S': \"Ahh the sounds of nature is so relaxing!!! I absolutely love this park. I think it runs for about 3-4 miles which is a nice walk. But unfortunately the end doesn't meet the start (or atleast i dont think) so I have to walk all the way back on the street side to where I started. Great family park to walk your dog or ride bikes or even exercise, its so relaxing. They also have benches to just sit down and watch the ducks in the water. They also have multiple scattered manmade beaches for kids to swim but I wouldn't go in that water Lol. Overall if your looking for a nice park to go to I would choose here.\"}],\n",
       " 'keywords': [('MountainEscapes', 0.7821349739331453),\n",
       "  ('OffTheBeatenPath', 0.7784539665385286),\n",
       "  ('RoadTrips', 0.7731180702441782)]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cypher_entities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/cypher_entities.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cypher_entities, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
