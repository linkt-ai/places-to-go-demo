{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Labeled Dataset\n",
    "\n",
    "This notebook will cover the cleaning and labelling of our social media dataset. We are using the [Instagram Images with Captions](https://www.kaggle.com/datasets/prithvijaunjale/instagram-images-with-captions) dataset for this training task. We will start the process by first cleaning the dataset down to around 200 rows, with a good variety of content and length in our selections. This will ensure that we have a good sample space for training the model from. Once the dataset is cleaned (trimmed down to 200 rows) we will need to label each caption according to the keywords that we are using for the project. The labelling will be done initially by GPT-4-Turbo, and then we will manually review and edit the labels using an excel sheet before we use the dataset to fine-tune our Bert Classifier.\n",
    "\n",
    "This notebook will cover: \n",
    "- **Data Cleaning**\n",
    "- **GPT Based Automatic Labeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "To begin, we need to load our dataset into a dataframe and filter out all the rows that we will not be using. There are over 20k rows, so we will have plenty of captions to choose from. Our raw dataset is stored in the `../data/social/captions.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random state for trimming rows. DO NOT CHANGE, as this will affect the values of the dataset\n",
    "RANDOM_STATE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3454.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>30.450782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>35.244671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>402.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word_count\n",
       "count  3454.000000\n",
       "mean     30.450782\n",
       "std      35.244671\n",
       "min      12.000000\n",
       "25%      14.000000\n",
       "50%      20.000000\n",
       "75%      32.000000\n",
       "max     402.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/social/captions.csv\")\n",
    "\n",
    "# Drop unused columns and rename columns for consistency\n",
    "df.drop(columns=[\"Sr No\", \"Image File\"], inplace=True)\n",
    "df.dropna(subset=['Caption'], inplace=True)\n",
    "df.rename(columns={\"Caption\": \"caption\"}, inplace=True)\n",
    "\n",
    "# Set a word count column\n",
    "df['word_count'] = df['caption'].str.split().str.len()\n",
    "\n",
    "# Filter out all rows with less than 10 words\n",
    "df = df[df['word_count'] >= 12]\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_list = df['caption'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/venues/yelp.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    venues = json.load(f)\n",
    "    # Mash the content of the reviews into a single string for each location\n",
    "    reviews_list = []\n",
    "    for location in venues:\n",
    "        review_content = \"\"\n",
    "        for review in location[\"reviews\"]:\n",
    "            review_text = next(iter(review.values()))\n",
    "            review_content += review_text + \" \"\n",
    "        reviews_list.append(review_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have extracted a set of 250 values from the dataset, while preserving the varied distribution of the caption length. Now, we will move on to the labelling process.\n",
    "\n",
    "### Setting up a Topic Extraction Pipeline\n",
    "\n",
    "Now, we want to extract relevant topics from each of our dataset seperately, and then view the combined results to settle on a good list of keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keyword Extraction and Scoring**\n",
    "\n",
    "We can begin by using TF-IDF NLP techniques, to extract keywords from each corpus. TD-IDF (\"Term Frequency-Inverse Document Frequency) can be described as:\n",
    "\n",
    "1. **Term Frequency (TF)**: This is a measure of how frequently a term appears in a document (in this case, a social media caption). It's calculated by dividing the number of times the term appears in the document by the total number of terms in the document. This reflects how important a term is within that specific document.\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**: This measures the importance of the term across the entire corpus (your collection of social media captions). It's calculated by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient. This step reduces the weight of terms that appear very frequently across the corpus, as these terms are less informative (common terms like 'the', 'is', etc.).\n",
    "\n",
    "3. **TF-IDF Score**: This is simply the product of TF and IDF. It's a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The intuition here is that a term is important for a document if it appears frequently in that document but not in many other documents. Thus, TF-IDF tends to filter out common terms that appear in many documents (like generic words) and highlight terms that are more unique to specific documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jackmoffatt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jackmoffatt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from typing import List\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def preprocess(caption: List[str]) -> List[str]:\n",
    "    \"\"\"Preprocesses the captions by removing special characters and converting to lowercase\"\"\"\n",
    "    caption = re.sub(r'[^\\w\\s]', '', caption.lower())\n",
    "    tokens = word_tokenize(caption)\n",
    "\n",
    "    # Remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "    # Remove non-alphabetic tokens and stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in stripped if word.isalpha() and word not in stop_words]\n",
    "    return words\n",
    "\n",
    "preprocessed_captions = [\" \".join(preprocess(caption)) for caption in captions_list]\n",
    "preprocessed_reviews = [\" \".join(preprocess(review)) for review in reviews_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Caption Keywords:\n",
      "love: 96.71974191882008\n",
      "thank: 69.25418802744699\n",
      "im: 65.9716268084095\n",
      "new: 64.68693526132691\n",
      "happy: 56.001911154939414\n",
      "birthday: 49.49981927072941\n",
      "much: 45.18046579772852\n",
      "see: 44.94033748513356\n",
      "cant: 44.32868285776308\n",
      "collection: 44.240309430604306\n",
      "\n",
      "Top 10 Review Keywords:\n",
      "love: 96.71974191882008\n",
      "thank: 69.25418802744699\n",
      "im: 65.9716268084095\n",
      "new: 64.68693526132691\n",
      "happy: 56.001911154939414\n",
      "birthday: 49.49981927072941\n",
      "much: 45.18046579772852\n",
      "see: 44.94033748513356\n",
      "cant: 44.32868285776308\n",
      "collection: 44.240309430604306\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def score_and_sort(documents: List[str]) -> List[Tuple[str, float]]:\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(preprocessed_captions)\n",
    "\n",
    "    # Get feature names and TF-IDF score of each word\n",
    "    scores = zip(vectorizer.get_feature_names_out(), tfidf_matrix.sum(axis=0).tolist()[0])\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sorted_scores\n",
    "\n",
    "sorted_caption_scores = score_and_sort(preprocessed_captions)\n",
    "sorted_review_scores = score_and_sort(preprocessed_reviews)\n",
    "\n",
    "# Display top N Caption keywords\n",
    "top_n = 10\n",
    "print(f\"Top {top_n} Caption Keywords:\")\n",
    "for feature, score in sorted_caption_scores[:top_n]:\n",
    "    print(f\"{feature}: {score}\")\n",
    "\n",
    "# Display top N Review keywords\n",
    "print(f\"\\nTop {top_n} Review Keywords:\")\n",
    "for feature, score in sorted_review_scores[:top_n]:\n",
    "    print(f\"{feature}: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic Modeling Each Corpus**\n",
    "\n",
    "Now that we have completed the TF-IDF process, we can move onto Topic Modeling. Topic modeling can uncover underlying themes or topics in large collections of text, such as your dataset of social media captions. One of the most popular methods for topic modeling is Latent Dirichlet Allocation (LDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_topics(documents: List[str], n_components: int = 10, top_n: int = 10) -> List[str]:\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    count_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, random_state=RANDOM_STATE)\n",
    "    lda.fit(count_matrix)\n",
    "\n",
    "    # Get feature names and TF-IDF score of each word\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topic_keywords = []\n",
    "    for topic in lda.components_:\n",
    "        topic_keywords.append([feature_names[i] for i in topic.argsort()[-top_n:]])\n",
    "\n",
    "    return topic_keywords\n",
    "\n",
    "num_topics = 15\n",
    "top_n = 25\n",
    "caption_topics = get_topics(preprocessed_captions, n_components=num_topics, top_n=top_n)\n",
    "review_topics = get_topics(preprocessed_reviews, n_components=num_topics, top_n=top_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 Caption Corpus Topics:\n",
      "Topic 0: ['people', 'guys', 'better', 'heart', 'going', 'lol', 'mom', 'dont', 'grateful', 'world', 'time', 'family', 'make', 'ive', 'years', 'today', 'thank', 'best', 'know', 'im', 'day', 'birthday', 'life', 'happy', 'love']\n",
      "Topic 1: ['party', 'ready', 'friends', 'way', 'come', 'god', 'amazing', 'time', 'life', 'sweet', 'little', 'baby', 'guys', 'best', 'big', 'day', 'dont', 'like', 'im', 'girl', 'know', 'happy', 'birthday', 'love', 'thank']\n",
      "Topic 2: ['use', 'time', 'heart', 'amazing', 'miami', 'bronze', 'available', 'want', 'end', 'greatest', 'concealer', 'mom', 'tell', 'shades', 'thank', 'new', 'like', 'kits', 'love', 'highlight', 'better', 'im', 'kkwbeautycom', 'powder', 'contour']\n",
      "Topic 3: ['ive', 'true', 'team', 'styled', 'dream', 'amazing', 'believe', 'photo', 'night', 'day', 'came', 'like', 'loved', 'la', 'vogue', 'new', 'im', 'come', 'shot', 'make', 'cover', 'love', 'thank', 'hair', 'shoot']\n",
      "Topic 4: ['best', 'saw', 'hope', 'isnt', 'world', 'let', 'make', 'thanks', 'got', 'im', 'guys', 'feel', 'really', 'story', 'friends', 'lot', 'think', 'day', 'like', 'want', 'life', 'thank', 'time', 'love', 'people']\n",
      "Topic 5: ['went', 'link', 'world', 'body', 'didnt', 'wait', 'follow', 'tune', 'love', 'cover', 'really', 'movie', 'kardashians', 'need', 'time', 'life', 'keeping', 'new', 'tonight', 'year', 'people', 'thank', 'help', 'dont', 'im']\n",
      "Topic 6: ['photo', 'kylie', 'video', 'share', 'thank', 'paris', 'app', 'week', 'kkwfragrancecom', 'im', 'sure', 'fragrance', 'ive', 'way', 'weekend', 'excited', 'wait', 'fun', 'fashion', 'pics', 'episode', 'amazing', 'met', 'make', 'tonight']\n",
      "Topic 7: ['kit', 'kkw', 'favorite', 'app', 'launch', 'matte', 'love', 'palette', 'launches', 'tomorrow', 'today', 'im', 'excited', 'check', 'kylie', 'kkwbeautycom', 'friday', 'wait', 'kyliecosmeticscom', 'guys', 'lip', 'launching', 'pst', 'collection', 'new']\n",
      "Topic 8: ['people', 'tonight', 'lover', 'selfie', 'ps', 'thats', 'open', 'house', 'make', 'thank', 'look', 'love', 'cute', 'right', 'song', 'hard', 'im', 'app', 'music', 'work', 'new', 'watch', 'video', 'link', 'bio']\n",
      "Topic 9: ['scooter', 'photo', 'night', 'dont', 'amazing', 'products', 'looking', 'play', 'line', 'know', 'park', 'coming', 'guys', 'today', 'ta', 'wait', 'stadium', 'soon', 'pic', 'like', 'thanks', 'time', 'im', 'love', 'got']\n",
      "Topic 10: ['award', 'hair', 'work', 'dress', 'kanye', 'come', 'look', 'honor', 'day', 'mugler', 'coming', 'shout', 'thank', 'beautiful', 'got', 'pic', 'dream', 'gardenia', 'check', 'lol', 'fashion', 'today', 'crystal', 'night', 'im']\n",
      "Topic 11: ['pics', 'shipping', 'limited', 'download', 'time', 'cotton', 'check', 'follow', 'pst', 'kimkardashianwestcom', 'join', 'black', 'free', 'shop', 'im', 'store', 'skimscom', 'excited', 'today', 'bio', 'link', 'new', 'collection', 'available', 'app']\n",
      "Topic 12: ['classic', 'hair', 'products', 'used', 'amazing', 'im', 'palette', 'coming', 'liner', 'artist', 'collection', 'kkw', 'nude', 'make', 'lip', 'lips', 'eye', 'love', 'mario', 'available', 'wearing', 'new', 'kkwbeautycom', 'look', 'glam']\n",
      "Topic 13: ['night', 'voting', 'think', 'making', 'women', 'vote', 'baby', 'picture', 'lol', 'youre', 'good', 'tag', 'best', 'time', 'people', 'life', 'dont', 'like', 'day', 'right', 'got', 'little', 'thank', 'pic', 'love']\n",
      "Topic 14: ['gon', 'na', 'good', 'like', 'big', 'old', 'going', 'brother', 'beautiful', 'new', 'dad', 'world', 'mom', 'sister', 'year', 'youre', 'baby', 'proud', 'know', 'friend', 'best', 'birthday', 'happy', 'im', 'love']\n",
      "\n",
      "Top 15 Venue Corpus Topics:\n",
      "Topic 0: ['come', 'dining', 'recommend', 'got', 'came', 'definitely', 'ordered', 'try', 'really', 'nice', 'dinner', 'amazing', 'wine', 'time', 'like', 'photos', 'experience', 'place', 'menu', 'delicious', 'restaurant', 'good', 'service', 'great', 'food']\n",
      "Topic 1: ['got', 'like', 'make', 'best', 'highly', 'super', 'game', 'escape', 'staff', 'review', 'group', 'definitely', 'really', 'amazing', 'recommend', 'class', 'room', 'thank', 'information', 'fun', 'great', 'experience', 'time', 'owner', 'business']\n",
      "Topic 2: ['room', 'came', 'went', 'definitely', 'love', 'staff', 'got', 'club', 'come', 'friends', 'nice', 'dont', 'really', 'drink', 'fun', 'people', 'time', 'like', 'good', 'music', 'night', 'drinks', 'great', 'place', 'bar']\n",
      "Topic 3: ['rooftop', 'burger', 'drink', 'delicious', 'come', 'definitely', 'didnt', 'nice', 'fries', 'bar', 'really', 'menu', 'chicken', 'order', 'like', 'time', 'came', 'got', 'ordered', 'drinks', 'place', 'great', 'service', 'good', 'food']\n",
      "Topic 4: ['course', 'clean', 'games', 'food', 'children', 'mini', 'old', 'really', 'room', 'activities', 'golf', 'adults', 'area', 'family', 'like', 'birthday', 'little', 'great', 'staff', 'play', 'party', 'time', 'place', 'fun', 'kids']\n",
      "Topic 5: ['selection', 'books', 'small', 'pottery', 'parking', 'good', 'en', 'pinball', 'love', 'lot', 'paint', 'staff', 'come', 'que', 'machines', 'game', 'great', 'store', 'el', 'la', 'like', 'time', 'arcade', 'place', 'games']\n",
      "Topic 6: ['ive', 'flavor', 'soup', 'time', 'try', 'definitely', 'order', 'came', 'got', 'service', 'great', 'pizza', 'delicious', 'spicy', 'fried', 'really', 'ordered', 'restaurant', 'sauce', 'rice', 'like', 'chicken', 'place', 'good', 'food']\n",
      "Topic 7: ['didnt', 'make', 'amazing', 'school', 'year', 'party', 'thank', 'asked', 'work', 'recommend', 'best', 'staff', 'got', 'owner', 'customer', 'told', 'said', 'like', 'wedding', 'people', 'day', 'business', 'service', 'experience', 'time']\n",
      "Topic 8: ['really', 'cool', 'area', 'artists', 'street', 'interesting', 'space', 'shop', 'new', 'place', 'floor', 'house', 'time', 'like', 'parking', 'building', 'history', 'miami', 'exhibits', 'free', 'visit', 'exhibit', 'photos', 'art', 'museum']\n",
      "Topic 9: ['pictures', 'worth', 'museum', 'place', 'beautiful', 'friends', 'photo', 'great', 'gardens', 'interactive', 'cool', 'room', 'recommend', 'visit', 'youre', 'different', 'tickets', 'magic', 'fun', 'like', 'really', 'time', 'photos', 'garden', 'experience']\n",
      "Topic 10: ['glad', 'hi', 'manager', 'hear', 'really', 'love', 'delicious', 'happy', 'enjoyed', 'like', 'hope', 'soon', 'experience', 'review', 'place', 'read', 'time', 'good', 'thank', 'great', 'service', 'food', 'information', 'owner', 'business']\n",
      "Topic 11: ['natural', 'want', 'water', 'need', 'new', 'vendors', 'know', 'buy', 'fish', 'prices', 'dont', 'love', 'cave', 'market', 'sea', 'items', 'great', 'selection', 'products', 'people', 'like', 'place', 'shop', 'beach', 'store']\n",
      "Topic 12: ['city', 'center', 'view', 'way', 'theres', 'day', 'views', 'nature', 'lot', 'little', 'like', 'time', 'people', 'water', 'hike', 'walk', 'photos', 'great', 'nice', 'area', 'beautiful', 'parking', 'place', 'trail', 'park']\n",
      "Topic 13: ['people', 'best', 'went', 'city', 'highly', 'chicago', 'took', 'got', 'amazing', 'ride', 'bus', 'history', 'like', 'really', 'way', 'day', 'fun', 'trip', 'tours', 'recommend', 'experience', 'guide', 'great', 'time', 'tour']\n",
      "Topic 14: ['car', 'people', 'inside', 'trains', 'holocaust', 'amazing', 'lighthouse', 'fountain', 'history', 'like', 'home', 'tower', 'beautiful', 'great', 'free', 'house', 'kayak', 'lloyd', 'photos', 'place', 'wright', 'cars', 'train', 'frank', 'chicago']\n"
     ]
    }
   ],
   "source": [
    "# Display top N Caption topics\n",
    "print(f\"Top {num_topics} Caption Corpus Topics:\")\n",
    "for i, topic in enumerate(caption_topics):\n",
    "    print(f\"Topic {i}: {topic}\")\n",
    "\n",
    "# Display top N Review topics\n",
    "print(f\"\\nTop {num_topics} Venue Corpus Topics:\")\n",
    "for i, topic in enumerate(review_topics):\n",
    "    print(f\"Topic {i}: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "With our topics properly modeled, we now can pass this list to Chat GPT for assistance in using it to create \"Keywords\" for relating Venues and Social Media posts. After discussing pros and cons with Chat GPT, I came to the decision that it will be best to use \"Personas\" instead of just keywords. As a result, ChatGPT was able to extract 8 common personas across the two corpa of text data. After many interations, this is the final Persona list we came up with:\n",
    "```json\n",
    "{\n",
    "    \"socialButterfly\": \"The Social Butterfly: A vibrant and outgoing individual who thrives in the energy of social gatherings, frequently found enjoying the nightlife at lively bars and clubs, and always up for a celebration with friends.\",\n",
    "    \"culinaryExplorer\": \"The Culinary Explorer: A gourmet aficionado who revels in culinary adventures, exploring diverse cuisines at fine dining establishments, and sharing their love for unique and delicious food experiences.\",\n",
    "    \"beautyFashionAficionado\": \"The Beauty and Fashion Aficionado: A trendsetter passionate about the latest in fashion and beauty, often seen at stylish shopping venues and beauty product launches, and always keeping up with the newest trends.\",\n",
    "    \"familyOrientedIndividual\": \"The Family-Oriented Individual: A person who cherishes family time and creates memories with loved ones, often participating in family-friendly activities, visiting parks, and enjoying experiences that cater to all ages.\",\n",
    "    \"artCultureEnthusiast\": \"The Art and Culture Enthusiast: A lover of the arts and culture, often found absorbing the rich experiences offered by museums and galleries, and always seeking to expand their horizons through artistic and cultural exploration.\",\n",
    "    \"wellnessSelfCareAdvocate\": \"The Wellness and Self-Care Advocate: A seeker of tranquility and personal well-being, often indulging in self-care routines, visiting wellness retreats and spas, and embracing serene natural environments for relaxation.\",\n",
    "    \"adventurerExplorer\": \"The Adventurer and Explorer: An intrepid soul with a thirst for adventure, often embarking on exciting journeys, exploring the great outdoors, and engaging in activities that offer a rush of adrenaline and connection with nature.\",\n",
    "    \"ecoConsciousConsumer\": \"The Eco-Conscious Consumer: A dedicated advocate for sustainability and eco-friendly living, preferring to shop at environmentally conscious stores, visit farmers' markets, and support initiatives that align with their green lifestyle.\"\n",
    "}\n",
    "```\n",
    "And with this, we have our 8 labels. These personas will be used as the 8 classification heads of the social media sentiment model, as well as the embedding terms to use for Venue's when creating Venue-Persona relationships. This data can be found in the `../data/personas.json` directory.\n",
    "\n",
    "As a final step, we will create a pickled dataframe with our personas, so we can perform similarity searches.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "personas = {\n",
    "    \"socialButterfly\": \"The Social Butterfly: A vibrant and outgoing individual who thrives in the energy of social gatherings, frequently found enjoying the nightlife at lively bars and clubs, and always up for a celebration with friends.\",\n",
    "    \"culinaryExplorer\": \"The Culinary Explorer: A gourmet aficionado who revels in culinary adventures, exploring diverse cuisines at fine dining establishments, and sharing their love for unique and delicious food experiences.\",\n",
    "    \"beautyFashionAficionado\": \"The Beauty and Fashion Aficionado: A trendsetter passionate about the latest in fashion and beauty, often seen at stylish shopping venues and beauty product launches, and always keeping up with the newest trends.\",\n",
    "    \"familyOrientedIndividual\": \"The Family-Oriented Individual: A person who cherishes family time and creates memories with loved ones, often participating in family-friendly activities, visiting parks, and enjoying experiences that cater to all ages.\",\n",
    "    \"artCultureEnthusiast\": \"The Art and Culture Enthusiast: A lover of the arts and culture, often found absorbing the rich experiences offered by museums and galleries, and always seeking to expand their horizons through artistic and cultural exploration.\",\n",
    "    \"wellnessSelfCareAdvocate\": \"The Wellness and Self-Care Advocate: A seeker of tranquility and personal well-being, often indulging in self-care routines, visiting wellness retreats and spas, and embracing serene natural environments for relaxation.\",\n",
    "    \"adventurerExplorer\": \"The Adventurer and Explorer: An intrepid soul with a thirst for adventure, often embarking on exciting journeys, exploring the great outdoors, and engaging in activities that offer a rush of adrenaline and connection with nature.\",\n",
    "    \"ecoConsciousConsumer\": \"The Eco-Conscious Consumer: A dedicated advocate for sustainability and eco-friendly living, preferring to shop at environmentally conscious stores, visit farmers' markets, and support initiatives that align with their green lifestyle.\"\n",
    "}\n",
    "\n",
    "\n",
    "data = [{'persona': persona, \"description\": description} for persona, description in personas.items()]\n",
    "df = pd.DataFrame(data=data)\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def embed_terms(terms: List[str]) -> List[List[float]]:\n",
    "    \"\"\"Embeds the given terms using the OpenAI API\"\"\"\n",
    "    response = client.embeddings.create(input=terms, model=\"text-embedding-ada-002\")\n",
    "    return [datum.embedding for datum in response.data]\n",
    "\n",
    "df['embeddings'] = df['description'].apply(lambda x: embed_terms([x])[0])\n",
    "df.to_pickle(\"../data/persona_dataframe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
