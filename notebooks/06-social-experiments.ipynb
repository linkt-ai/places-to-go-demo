{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "DATA_DIR = \"../data/social\"\n",
        "\n",
        "# Add parent directory to path so we can import from 'vectorstores\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "def data_file(name: str) -> str: return f\"{DATA_DIR}/{name}.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from vectorstores.keywords import KeywordVectorstore\n",
        "openai_embeddings = KeywordVectorstore()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from functools import reduce\n",
        "\n",
        "import torch  \n",
        "import torch.nn.functional as F\n",
        "from numpy.linalg import norm\n",
        "\n",
        "from transformers import BertTokenizer,BertModel\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 1: Does segmenting the input post result in better embeddings? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [],
      "source": [
        "def post_query_term(row: pd.Series, segment=False):\n",
        "    if segment: \n",
        "        return f\"Author Name: {row['author_name']}. Post Description: {row['description']}. Author Description: {row['author_signature']}.\"\n",
        "    return f\"{row['author_name']} - {row['description']} - {row['author_signature']}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [],
      "source": [
        "search_terms_default = df.apply(post_query_term, axis=1).tolist()\n",
        "search_terms_segmented = df.apply(post_query_term, args={\"segment\":True}, axis=1).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.55"
            ]
          },
          "execution_count": 179,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# returns oai keywords for post, oai keywords for segmented post, intersection between the two\n",
        "def compare_segmented_input(idx):\n",
        "\n",
        "    post, segmented_post = search_terms_default[idx], search_terms_segmented[idx]\n",
        "    post_keywords, segmented_post_keywords = openai_embeddings.get_keywords([post, segmented_post])\n",
        "    post_keywords, segmented_post_keywords = set([p[0] for p in post_keywords]), set([p[0] for p in segmented_post_keywords])\n",
        "    return post_keywords, segmented_post_keywords, post_keywords.intersection(segmented_post_keywords)\n",
        "\n",
        "\n",
        "#calculate the average overlap between keywords for normal post and segmented post. \n",
        "def test_compare_segmented_input(num_cases):\n",
        "    overlaps = []\n",
        "    for idx in range(num_cases): \n",
        "        _,_, overlap = compare_segmented_input(idx)\n",
        "        overlaps.append(len(overlap))\n",
        "    return np.mean(overlaps)\n",
        "\n",
        "# test_compare_segmented_input(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion \n",
        "* over 20 cases, an average of 2.6/3 keywords found for segmented post are also found for non segmented post.  \n",
        "* segmenting the posts does not lead to different classification results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 2: Does BERT provide comparable embeddings? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BertEmbeddings: \n",
        "    def __init__(self) -> None:\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n",
        "        self.model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.vs = self.build_vector_store()\n",
        "    \n",
        "    def embed(self, s, normalize_embedding = False, return_list = True):\n",
        "        inputs = self.tokenizer(s, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "        with torch.no_grad(): \n",
        "            output = self.model(**inputs)\n",
        "        pool_embedding, last_hidden = output.pooler_output, output.last_hidden_state\n",
        "\n",
        "        if return_list: \n",
        "            pool_embedding = pool_embedding.squeeze(0).tolist()\n",
        "            # if normalize_embedding: \n",
        "            #     pool_embedding = self.normalize(pool_embedding)\n",
        "    \n",
        "        return pool_embedding\n",
        "\n",
        "    def build_vector_store(self, as_df = True):\n",
        "        # Creating embeddings for all keywords in the keywords list.\n",
        "\n",
        "        with open('../data/personas.json', 'r') as file: \n",
        "            personas = json.load(file)\n",
        "\n",
        "        personas = reduce(lambda l1, l2: l1 + l2, personas.values())\n",
        "        vs = {i: {'text': v, 'embedding': self.embed(v)} for i, v in enumerate(personas)}\n",
        "        \n",
        "        if as_df: \n",
        "            vs = pd.DataFrame.from_dict(vs, orient='index')\n",
        "\n",
        "        return vs \n",
        "    \n",
        "    def search_vector_store(self, sentence, k = 5):\n",
        "        sent_embedding = self.embed(sentence)\n",
        "        sent_embedding_normalized = norm(sent_embedding)\n",
        "        vs = self.vs \n",
        "        vs[\"score\"] = vs.embedding.apply(lambda x: np.dot(sent_embedding,x)/(sent_embedding_normalized*norm(x)))\n",
        "  \n",
        "        vs.sort_values(by=\"score\", ascending=False, inplace=True)\n",
        "        top_keywords = [\n",
        "            (row[\"text\"], row[\"score\"])\n",
        "            for index, row in vs[[\"text\", \"score\"]].head(k).iterrows()\n",
        "        ]\n",
        "        return top_keywords\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "bert_embeddings = BertEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 185,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# compare openai embeddings with bert embeddings\n",
        "def compare_oai_bert(post): \n",
        "    oai = openai_embeddings.get_keywords([post])[0]\n",
        "    bert = bert_embeddings.search_vector_store(post)\n",
        "    oai = set([pred[0] for pred in oai])\n",
        "    bert = set([pred[0] for pred in bert])\n",
        "\n",
        "    return oai, bert, oai.intersection(bert) \n",
        "\n",
        "def test_compare_oai_bert(num_cases):\n",
        "    overlaps = []\n",
        "    for idx in range(num_cases): \n",
        "        _, _, overlap = compare_oai_bert(search_terms_default[idx])\n",
        "        overlaps.append(len(overlap))\n",
        "    return np.mean(overlaps)\n",
        "\n",
        "# test_compare_oai_bert(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion \n",
        "* On average, 0 of 3 classes found by openAI embeddings are found by BERT embeddings. \n",
        "* The openAI embeddings out perform BERT embeddings, based on anecdotal comparisons of resultant classifications. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 3: Does HuggingFace multi-label classification work out of the box, or does it require fine-tuning? \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BertClassifier: \n",
        "    def __init__(self) -> None:\n",
        "        bert_ckpt = \"distilbert-base-uncased\" \n",
        "        self.keywords = self.get_keywords()\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(bert_ckpt)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            bert_ckpt,\n",
        "            num_labels=len(self.keywords)  ,\n",
        "            problem_type=\"multi_label_classification\",  \n",
        "        )\n",
        "        self.model.config.id2label =  self.keywords\n",
        "\n",
        "    def get_keywords(self):\n",
        "        with open('../data/keywords.json', 'r') as file:\n",
        "            keywords = json.load(file)\n",
        "        keywords = reduce(lambda l1, l2: l1 + l2, keywords.values())\n",
        "        keywords = {i: v for i, v in enumerate(keywords)}\n",
        "        return keywords \n",
        "    \n",
        "    def finetune(self, data):\n",
        "        # TODO: fine tune the classifier with some data \n",
        "        pass \n",
        "    \n",
        "    def classify(self, sequence, threshold=0.53):\n",
        "        inputs = self.tokenizer(sequence, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(**inputs).logits\n",
        "        probabilities = F.sigmoid(logits)\n",
        "\n",
        "        label_indices = (probabilities > threshold).nonzero(as_tuple=True)[1]\n",
        "        assigned_labels = [label_indices[i].item() for i in range(len(label_indices))]\n",
        "        label_ids = [self.model.config.id2label[idx] for idx in assigned_labels]\n",
        "\n",
        "        return probabilities, label_ids "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "bert_classifier = BertClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Input Post:\n",
            "<START> \n",
            "onzpo - Replying to @patrick_sea567 Nah, that's CAP. It's like I always say, I #onzpo use #vfx to make it look like I'm using a #noeyefilter but no #ihaveeyes check the  #lore lol - I'm glad you're still around!\n",
            "Smash Follow!🕹️\n",
            "Business:chris@clickstalent.com \n",
            "<END>, \n",
            "Method 1) OpenAI embedding similarity: , \n",
            "Method 2) BERT multihead classification: ['Artisanal', 'Live Music', 'Exploration', 'Fitness Challenge'], \n",
            "          BERT classification scores: tensor([[0.5277, 0.5781, 0.4931, 0.5230, 0.5258, 0.4869, 0.4891, 0.5119, 0.4926,\n",
            "         0.4927, 0.5279, 0.4953, 0.4789, 0.4848, 0.5216, 0.4852, 0.4677, 0.4983,\n",
            "         0.5171, 0.5283, 0.5042, 0.5502, 0.5119, 0.4958, 0.4993, 0.4931, 0.5232,\n",
            "         0.5018, 0.4678, 0.5101, 0.5196, 0.4828, 0.4945, 0.4950, 0.4952, 0.5045,\n",
            "         0.4810, 0.4678, 0.5083, 0.4845, 0.5541, 0.4929, 0.5120, 0.4562, 0.4727,\n",
            "         0.5134, 0.4990, 0.5245, 0.4586, 0.4996, 0.5507, 0.5054, 0.5133, 0.4752,\n",
            "         0.4887, 0.4950, 0.5140, 0.4981, 0.4792, 0.5086]]), \n",
            "          \n",
            "          \n",
            "\n",
            "Input Post:\n",
            "<START> \n",
            "Darryl - Epic Street Interview Fail 🙆🏾‍♂️🙅🏾‍♂️ @Owé Collections #streetinterview #streetinterviews #streetprank #prank  - Street Interview - Saturdays @11am EST\n",
            "📧: info@darrylskits.com\n",
            "💌: $darrylskits \n",
            "<END>, \n",
            "Method 1) OpenAI embedding similarity: , \n",
            "Method 2) BERT multihead classification: ['Artisanal', 'Exotic Flavors', 'Sweet Treats', 'Live Music', 'Exploration', 'Fitness Challenge'], \n",
            "          BERT classification scores: tensor([[0.5224, 0.5700, 0.5010, 0.5349, 0.5148, 0.4956, 0.4947, 0.5173, 0.4936,\n",
            "         0.4895, 0.5314, 0.4933, 0.4827, 0.4854, 0.5223, 0.4871, 0.4811, 0.4956,\n",
            "         0.5157, 0.5164, 0.5105, 0.5430, 0.5162, 0.4930, 0.5099, 0.4931, 0.5219,\n",
            "         0.4973, 0.4795, 0.5105, 0.5114, 0.4915, 0.4801, 0.4965, 0.4922, 0.5103,\n",
            "         0.4945, 0.4768, 0.5053, 0.5001, 0.5422, 0.4822, 0.5097, 0.4584, 0.4710,\n",
            "         0.5132, 0.5089, 0.5217, 0.4588, 0.4989, 0.5469, 0.5019, 0.5110, 0.4690,\n",
            "         0.4789, 0.4810, 0.5141, 0.4956, 0.4917, 0.5050]]), \n",
            "          \n",
            "          \n"
          ]
        }
      ],
      "source": [
        "def evaluate_methods(post):\n",
        "    bert_probabilities, bert_categories = bert_classifier.classify(post)\n",
        "    relevant_keywords = '' #vs.get_keywords([post])\n",
        "\n",
        "    print()\n",
        "    print(f'''Input Post:\\n<START> \\n{post} \\n<END>, \n",
        "Method 1) OpenAI embedding similarity: {relevant_keywords}, \n",
        "Method 2) BERT multihead classification: {bert_categories}, \n",
        "          BERT classification scores: {bert_probabilities}, \n",
        "          \n",
        "          ''')\n",
        "# evaluate_methods(search_terms_default[0])\n",
        "# evaluate_methods(search_terms_default[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion \n",
        "* The classifier requires fine-tuning, since it does not automatically embed the input classes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 4: Does fine-tuning HF multilabel classifier result in better classifications? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('../data/social/labeled_captions_gpt.csv')\n",
        "keywords = list(df.columns[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "bert_ckpt = \"distilbert-base-uncased\" \n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_ckpt, use_fast = True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    bert_ckpt,\n",
        "    num_labels=len(keywords)  ,\n",
        "    problem_type=\"multi_label_classification\",  \n",
        ")\n",
        "model.config.id2label =  keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PostDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = df.loc[index]\n",
        "        caption, classes = row[0], torch.tensor([row[1:]], dtype=torch.float32)\n",
        "        item =tokenizer(caption, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "        item['labels']  = classes\n",
        "        return item "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    # TODO: implement batching \n",
        "    pass \n",
        "\n",
        "\n",
        "ds = PostDataset(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean loss:  0.5207712499300639\n",
            "Mean loss:  0.4536612930893898\n",
            "Mean loss:  0.37426499197880425\n",
            "Mean loss:  0.29873875026901564\n",
            "Mean loss:  0.24966790671149888\n",
            "Mean loss:  0.22045469112694263\n",
            "Mean loss:  0.20574019826948642\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "epochs = 7\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr=1e-5, \n",
        "                  eps=1e-8)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(ds)*epochs)\n",
        "def train():\n",
        "    for i in range(epochs):\n",
        "        model.train()\n",
        "        loss_train_total = 0\n",
        "        for i in range(len(ds)):\n",
        "            \n",
        "            model.zero_grad()\n",
        "            inputs = ds[i]\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs[0]\n",
        "            loss_train_total += loss.item()\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "        print('Mean loss: ', loss_train_total/len(ds))\n",
        "train() \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime \n",
        "def save_model_state_dict(model, use_datetime = False, model_state_dict_id = None ):\n",
        "    if model_state_dict_id: \n",
        "        pass \n",
        "    elif use_datetime:\n",
        "        now = datetime.now()\n",
        "        model_state_dict_id = str(now).replace(\" \", \"\")\n",
        "    else:\n",
        "        return \n",
        "    \n",
        "    torch.save(model.state_dict(), f'../model_state_dicts/bert-ft-{model_state_dict_id}.model')\n",
        "\n",
        "def load_model_state_dict(model, model_state_dict_id):\n",
        "    model.load_state_dict(torch.load(f'../model_state_dicts/bert-ft-{model_state_dict_id}.model', map_location=torch.device('cpu')))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_model_state_dict(model, model_state_dict_id= '1')\n",
        "load_model_state_dict(model, model_state_dict_id='1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean accuracy: 0.775\n"
          ]
        }
      ],
      "source": [
        "def compute_accuracy(predicted, actual):\n",
        "    correct = (predicted == actual).sum().item()\n",
        "    accuracy = correct / actual.size(0)\n",
        "    return accuracy\n",
        "\n",
        "# returns 1.0 if the top k classes in the prediction match the k True classes in the input. \n",
        "def compute_example_based_accuracy(y_pred, y_actual):\n",
        "    k = torch.sum(y_actual == 1)  # number of classes assigned in the label\n",
        "    pred_classes, actual_classes = torch.topk(y_pred, k=k).indices, torch.where(y_actual == 1)[0]\n",
        "    return compute_accuracy(pred_classes, actual_classes) \n",
        "\n",
        "def evaluate(): \n",
        "    accuracies = []\n",
        "    for i in range(len(ds)):\n",
        "        with torch.no_grad():\n",
        "            inputs = ds[i]\n",
        "            outputs = model(**inputs) \n",
        "\n",
        "            y_predicted, y_actual = outputs.logits.squeeze(0), inputs['labels'].squeeze(0)\n",
        "            accuracy = compute_example_based_accuracy(y_predicted, y_actual)\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "    print(\"Mean accuracy:\", np.mean(accuracies))\n",
        "\n",
        "evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get top k predicted classes on input string \n",
        "def predict(input, k):\n",
        "    with torch.no_grad():\n",
        "        input =tokenizer(input, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "        output = model(**input)\n",
        "        pred_logits = output.logits.squeeze(0)\n",
        "        pred_label_ids = torch.topk(pred_logits, k).indices\n",
        "        pred_labels = [keywords[i] for i in pred_label_ids]\n",
        "        return pred_labels\n",
        "\n",
        "# prints models predictions on a list of strings \n",
        "def predict_for_test_dataset(test_data, k=3):\n",
        "    for t in test_data: \n",
        "        print(f\"Post: {t}\\nClasses: {predict(t, k)}\")\n",
        "        print('\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Post: Raw shrimp for thanksgiving #gaydolphin\n",
            "Classes: ['culinaryExplorer', 'familyOrientedIndividual', 'socialButterfly', 'wellnessSelfCareAdvocate']\n",
            "\n",
            "\n",
            "Post: nyc psychic ruined my life\n",
            "Classes: ['artCultureEnthusiast', 'adventurerExplorer', 'socialButterfly', 'ecoConsciousConsumer']\n",
            "\n",
            "\n",
            "Post: i didn't fall but i won't be ice skating again.\n",
            "Classes: ['artCultureEnthusiast', 'adventurerExplorer', 'wellnessSelfCareAdvocate', 'socialButterfly']\n",
            "\n",
            "\n",
            "Post: i fell but i will be ice skating again.\n",
            "Classes: ['adventurerExplorer', 'wellnessSelfCareAdvocate', 'artCultureEnthusiast', 'ecoConsciousConsumer']\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_examples_3 = [\"Raw shrimp for thanksgiving #gaydolphin\", \"nyc psychic ruined my life\", \"i didn't fall but i won't be ice skating again.\", \"i fell but i will be ice skating again.\"]\n",
        "predict_for_test_dataset(c, k =4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Post: I love the outdoors!\n",
            "Classes: ['ecoConsciousConsumer', 'adventurerExplorer', 'familyOrientedIndividual']\n",
            "\n",
            "\n",
            "Post: Danced all night with KP, then total girls night in talkin till mornin \n",
            "Classes: ['socialButterfly', 'wellnessSelfCareAdvocate', 'artCultureEnthusiast']\n",
            "\n",
            "\n",
            "Post: My London, madenessss \n",
            "Classes: ['beautyFashionAficionado', 'artCultureEnthusiast', 'socialButterfly']\n",
            "\n",
            "\n",
            "Post: One of the best UNICEF ambassadors\n",
            "Classes: ['beautyFashionAficionado', 'ecoConsciousConsumer', 'artCultureEnthusiast']\n",
            "\n",
            "\n",
            "Post: Taking my power back.. can't wait to show you where I've been. I love y'all. Ps, I still obsess over pickles. \n",
            "Classes: ['socialButterfly', 'wellnessSelfCareAdvocate', 'artCultureEnthusiast']\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_examples_1 = [\"I love the outdoors!\", \n",
        "                 \"Danced all night with KP, then total girls night in talkin till mornin \", \n",
        "                 \"My London, madenessss \", \n",
        "                 \"One of the best UNICEF ambassadors\",\n",
        "                 \"Taking my power back.. can't wait to show you where I've been. I love y'all. Ps, I still obsess over pickles. \" ]\n",
        "\n",
        "predict_for_test_dataset(test_examples_1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Post: Happy birthday brother. I can't wait to go to spain and go hiking in the alps.\n",
            "Classes: ['familyOrientedIndividual', 'adventurerExplorer', 'wellnessSelfCareAdvocate', 'socialButterfly']\n",
            "\n",
            "\n",
            "Post: Happy birthday brother. I can't wait to go to spain and eat food.\n",
            "Classes: ['culinaryExplorer', 'familyOrientedIndividual', 'socialButterfly', 'wellnessSelfCareAdvocate']\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_examples_2 = [\"Happy birthday brother. I can't wait to go to spain and go hiking in the alps.\", \"Happy birthday brother. I can't wait to go to spain and eat food.\" ]\n",
        "\n",
        "predict_for_test_dataset(test_examples_2, k=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Post: Happy birthday brother. I can't wait to go to spain and go hiking in the alps.\n",
            "Classes: ['familyOrientedIndividual', 'adventurerExplorer', 'wellnessSelfCareAdvocate', 'socialButterfly']\n",
            "\n",
            "\n",
            "Post: Happy birthday brother. I can't wait to go to spain and eat food.\n",
            "Classes: ['culinaryExplorer', 'familyOrientedIndividual', 'socialButterfly', 'adventurerExplorer']\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_examples_3 = [\"Raw shrimp for thanksgiving #gaydolphin\", \"nyc psychic ruined my life\", \"i didn't fall but i won't be ice skating again.\", \"i fell but i will be ice skating again.\"]\n",
        "predict_for_test_dataset(test_examples_2, k=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BertClassifier: \n",
        "    def __init__(self) -> None:\n",
        "        bert_ckpt = \"distilbert-base-uncased\" \n",
        "        self.keywords = self.get_keywords()\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(bert_ckpt)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            bert_ckpt,\n",
        "            num_labels=len(self.keywords)  ,\n",
        "            problem_type=\"multi_label_classification\",  \n",
        "        )\n",
        "        self.model.config.id2label =  self.keywords\n",
        "\n",
        "    def get_keywords(self):\n",
        "        with open('../data/keywords.json', 'r') as file:\n",
        "            keywords = json.load(file)\n",
        "        keywords = reduce(lambda l1, l2: l1 + l2, keywords.values())\n",
        "        keywords = {i: v for i, v in enumerate(keywords)}\n",
        "        return keywords \n",
        "    \n",
        "    def finetune(self, data):\n",
        "        # TODO: fine tune the classifier with some data \n",
        "        pass \n",
        "    \n",
        "    def classify(self, sequence, threshold=0.53):\n",
        "        inputs = self.tokenizer(sequence, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(**inputs).logits\n",
        "        probabilities = F.sigmoid(logits)\n",
        "\n",
        "        label_indices = (probabilities > threshold).nonzero(as_tuple=True)[1]\n",
        "        assigned_labels = [label_indices[i].item() for i in range(len(label_indices))]\n",
        "        label_ids = [self.model.config.id2label[idx] for idx in assigned_labels]\n",
        "\n",
        "        return probabilities, label_ids "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
