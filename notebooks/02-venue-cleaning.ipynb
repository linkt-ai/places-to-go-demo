{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Venue Data Cleaning\n",
    "\n",
    "Now that we have webscraped the reviews of our businesses, we need to clean the data so that it can be used to create Graph entites as well as documents for a pinecone index. The goal of this notebook is to populate two files:\n",
    "- `../data/cypher/venue-entities.json`: The data that will be used for writing the venue entites to Neo4J\n",
    "- `../data/vector/venue-documents.json`: The data that will be used for writing the venue documents to Pinecone\n",
    "\n",
    "## General Process\n",
    "\n",
    "**Data Loading**:\n",
    "We will start by reading out of the final file of the scraping process: `../data/scrape/locations_finished`. We will extract on the fields that we need to extract in order to create the embedding documents and the Venue entities.\n",
    "\n",
    "**Prompt Engineering**:\n",
    "We will start by engineering a prompt that has GPT-3.5-Turbo parse the Yelp Business and make certian classifications for that business. To do this, we will need to have a prompt template that characterizes a JSON schema that the model should output for each venue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Loading\n",
    "\n",
    "Let's start by loading our scraped location data, and extracting the necessary fields that we will need for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/scrape/reviews/locations_finished.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    location_data = json.load(f)\n",
    "    trimmed_location_data = []\n",
    "    for location in location_data:\n",
    "        trimmed_location_data.append({\n",
    "            \"id\": location['id'],\n",
    "            \"name\": location['name'],\n",
    "            \"city\": location['city_code'],\n",
    "            'rating': location['rating'],\n",
    "            \"reviews\": location['reviews'],\n",
    "            \"url\": location['url'],\n",
    "            \"image_url\": location['image_url'],\n",
    "            \"categories\": location['categories'],\n",
    "        })\n",
    "    location_data = trimmed_location_data\n",
    "    del trimmed_location_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Engineering\n",
    "\n",
    "There are tons of locations in our raw Yelp dataset that have no relevance to someone looking to make plans for an upcoming vaction or trip. Let's use ChatGPT to trim down these results, so that we can work with a list of venues that will all make for relevant activity suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTION_PROMPT = \"\"\"\n",
    "You are working for a travel agency. You are searching through Yelp Business Pages to find new and exciting vaction activites\n",
    "for your clients. Your clients are often young adults (18-35) who are looking for fun, exciting, and trendy vaction activites \n",
    "and resturants to visit. Your task is to select only the most premium and interesting businesses for your clients to visit.\n",
    "\n",
    "You will be given a JSON file containing a list of Yelp Businesses. For each business, you will be provided with the ID, name, \n",
    "categories, and yelp rating. Your task is to infer which of these businesses are interesting for your clients to visit based \n",
    "on this limited information. You are attempting to make your best guess based on the information you are provided with. \n",
    "\n",
    "The most IMPORTANT component of this task is removing all busineses that are not relevant for TOURISTS. Since your clients are planning\n",
    "a vacation, you should only select businesses that are relevant for a tourist to visit. You MUST FILTER OUT all businesses\n",
    "that are not relevant for tourism. \n",
    "\n",
    "To summarize your instructions:\n",
    "For any business that is not relevant for a toursit, you filter it out. It is crucial that \n",
    "you are very selective in your process, and you only produce a list of businesses that are relevant \n",
    "for a TOURIST to visit.\n",
    "\n",
    "Once you have made your decision on which businesses you are keeping, you should provide your list of relevant business IDs in a JSON \n",
    "object. Here is an example of the JSON object you should produce:\n",
    "{\n",
    "    \"relevant_businesses\": [\n",
    "        '0EWwmbkJ97nDZCPOdrWMau', \n",
    "        'fWdKIXkM66qoLrllGfT7LR',\n",
    "        'Sg4N2msDQjYXuqmaR5a9OO',\n",
    "        'Zvb2bH2rDtJpJwK0YTfpwy',\n",
    "        'EDE2n455bQndLRbeLMn5xb',\n",
    "        'A77ORqKZTOck45DDArSWoL', \n",
    "        'RNqVLeLK7YjrbiTBU7YIFu', \n",
    "        'ApKcaAL2HMxVtLvGrUVAPi', \n",
    "        'cjCiLUeU5pnU5mnG0lvcKN', \n",
    "        'NbUT2tBMs3c5cvIUmrUnCE', \n",
    "        'rxN806Yvca8Wb1E37WXEc4', \n",
    "        'eN3Y0CQm3Ht03Rz6wn9HBn'\n",
    "    ]\n",
    "}\n",
    "\n",
    "It is crucial that you adhere strictly to this format, otherwise, your submission will not be accepted.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class ParseError(Exception):\n",
    "    pass\n",
    "\n",
    "def parse_output(output):\n",
    "    try:\n",
    "        output_object = json.loads(output)\n",
    "        biz_list = output_object['relevant_businesses']\n",
    "        return biz_list\n",
    "    except:\n",
    "        raise ParseError(\"Could not parse output\")\n",
    "\n",
    "def select_businesses(batch):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': SELECTION_PROMPT},\n",
    "        {'role': 'user', 'content': json.dumps(batch)}\n",
    "    ]\n",
    "    completion = client.chat.completions.create(\n",
    "        model='gpt-3.5-turbo-1106',\n",
    "        messages=messages,\n",
    "        response_format={'type': 'json_object'}\n",
    "    )\n",
    "    biz_list = parse_output(completion.choices[0].message.content)\n",
    "    return biz_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [09:29<00:00,  6.06s/it]\n"
     ]
    }
   ],
   "source": [
    "# Format the Data For the Prompt\n",
    "prompt_data = [\n",
    "    {\n",
    "        'id': location['id'], \n",
    "        'name': location['name'], \n",
    "        'categories': location['categories'],\n",
    "        'rating': location['rating'],\n",
    "    } \n",
    "    for location in location_data\n",
    "]\n",
    "\n",
    "def save_progress(new_ids, new_failed):\n",
    "\n",
    "    # Read the current progress\n",
    "    with open(\"../data/trimming_progress.json\", 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        biz_ids = data['biz_ids'] + new_ids\n",
    "        failed = data['failed_businesses'] + new_failed\n",
    "        batch_count = data['batch_count'] + 1\n",
    "\n",
    "    # Update the current progress with the new results\n",
    "    with open(\"../data/trimming_progress.json\", 'w', encoding='utf-8') as f:\n",
    "        data = {\n",
    "            'biz_ids': biz_ids,\n",
    "            'failed_businesses': failed,\n",
    "            'batch_count': batch_count\n",
    "        }\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "for i in tqdm(range(0, len(prompt_data), 30)):\n",
    "    failed = []\n",
    "    try:\n",
    "        batch = prompt_data[i:i+30]\n",
    "        selected_ids = select_businesses(batch)\n",
    "    except ParseError:\n",
    "        failed = batch\n",
    "        \n",
    "    save_progress(selected_ids, failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the IDs provide by GPT to filter our list of locations\n",
    "# \n",
    "# with open(\"../data/trimming_progress.json\", 'r', encoding='utf-8') as f:\n",
    "#     data = json.load(f)\n",
    "#     valid_ids = data['biz_ids']\n",
    "\n",
    "# with open(\"../data/scrape/reviews/locations_finished.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     location_data = json.load(f)\n",
    "#     trimmed_location_data = []\n",
    "#     for location in location_data:\n",
    "#         if location['id'] in valid_ids:\n",
    "#             trimmed_location_data.append(location)\n",
    "#     location_data = trimmed_location_data\n",
    "#     del trimmed_location_data\n",
    "\n",
    "# with open(\"../data/trimmed_locations.json\", 'w') as f:\n",
    "#     json.dump(location_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Classification\n",
    "\n",
    "Now that we have our list trimmed down to only relevant vacation activites and resturants, we should look to begin classifying our businesses so that they can be stored in the graph and in a vector store. To store the businesses in the vector store, we will want to have ChatGPT write a 1 sentence description of each business. This will allow us to embed these descriptions and store those embeddings in a pinecone index. Whenever our AI travel agent wants to reccomend locations. It can first use the pinecone index to get a set of businesses that match the needs of the user's query, then it can analyze those businesses in the graph, to select those that are most appropriately algined with the user's mood board. \n",
    "\n",
    "For vector searching we want a clear concise one sentence description of the business. Then, when our agent is querying the vectorstore, it will write a query that is a clear concise one sentence description of the business it is looking for. \n",
    "\n",
    "For storing venues in the graph, we want to associate each venue with each of the 8 personas. To do this, we will need to develop a labeled dataset of reviews. Each reveiw should classify what labels are associated with the venue. Then, we will train a BERT model on an 8-head classification task. Then, when we want to classify venues, we will need to pass the venue's reveiws to the BERT classifier. This will give us a relevance score for each persona. This will create a graph architecture where every venue is related to every persona and every social media post is related to each persona. We will have our set of 8 personas ($P$) Given a set ($S$) of social media posts, we can use a user's query to first filter a set of potential venues ($V$). Each venue and each post will be related to every post through the 8 personas. We will compute the score of each venue $k_i \\forall i \\in \\{1, 2, ... |v| \\}$. We will compute $k_i$ as follows:\n",
    "$$\n",
    "k_i = \\sum_{s_k \\in S} \\sum_{j = 0}^{8} \\text{weight}(p_j, v_i) + \\text{weight}(p_j, s_k)\n",
    "$$\n",
    "\n",
    "Where $\\text{weight}$ is the function that takes two nodes and returns the weight of the relationship between those two nodes. From this summation, we can see that our venue suggestion results will first be filtered, such that we are only ranking venues that are appropriate based on the user's query and then are ranked by computing the relevance between the posts provided by the user and the venue we are ranking. \n",
    "\n",
    "**Data Processing**:\n",
    "To start, we need to write 1 sentence descriptions of the business for each business in our dataset. Let's use ChatGPT to read the page content of a business's Yelp page in order to return a 1 sentence description of that business. In addition to the one sentence description, we also need to use ChatGPT to extract a businesses hours from the Yelp page as well. This will allow us to filter our vector query using times, so that we only return businesses that are open during the window of time that the user is looking to book an event during.\n",
    "\n",
    "**Persona Classfiication**:\n",
    "Once we have our businesses processed by ChatGPT, we will want to use that information to feed a BERT Model with information on a buisness and receive label scores for each of our 8 labels. This way we will be able to draw relationships between each of our persona nodes and each of our venue nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "To start, there is plenty of data that can be extracted using Regex. We will begin be extracting the business hours from each HTML page, and appending the hours to the `location_data` object. This will allow our agent to filter out any businesses that are not open at the time of the event requested by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/scrape/locations_finished.json\", \"r\") as f:\n",
    "    location_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "business_hours = []\n",
    "\n",
    "for loc in location_data:\n",
    "    pattern = pattern = r\"(Mon|Tue|Wed|Thu|Fri|Sat|Sun)\\n(.*?)\\n\"\n",
    "    matches = re.findall(pattern, loc['page_content'], re.DOTALL)\n",
    "    hours = {}\n",
    "    for match in matches:\n",
    "        hours[match[0]] = match[1]\n",
    "    loc['hours'] = hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the hours collected, we can begin prepping our context for the LLM prompts. Since it would take way too many tokens to prompt the LLM with the entire webpage content of each business, we will instead use NLP techniques to first analyze the page of each business, and then create a much shorter keyword analysis to be used by the LLM (in conjunction with other minor details) to write a business_summary and make persona labels.\n",
    "\n",
    "We will use the persona labels to train a BERT encoder. The BERT encoder will be take a concatenated string of the keyword analysis as input, and output the persona labels. The classification scores of BERT will be used as the relationship weights between each venue and persona entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "content = [loc['page_content'].lower() for loc in location_data]\n",
    "\n",
    "# strip numbers out of each item in content\n",
    "content = [re.sub(r'\\d+', '', item) for item in content]\n",
    "\n",
    "# Adding custom common words to the stop words list\n",
    "custom_stop_words = stopwords.words('english') + ['yelp', '2023', 'reviews', 'photos', 'chicago', 'york', 'los', 'angeles', 'pheonix', 'ave', 'st', 'rd']\n",
    "\n",
    "# Initialize TF-IDF Vectorizer with updated parameters\n",
    "vectorizer = TfidfVectorizer(max_features=200, stop_words=custom_stop_words, min_df=2, max_df=0.5)\n",
    "\n",
    "# Fit and transform the Yelp texts\n",
    "tfidf_matrix = vectorizer.fit_transform(content)\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "top_n = 50\n",
    "keywords = []\n",
    "for doc in range(tfidf_matrix.shape[0]):\n",
    "    feature_index = tfidf_matrix[doc,:].nonzero()[1]\n",
    "    tfidf_scores = zip(feature_index, [tfidf_matrix[doc, x] for x in feature_index])\n",
    "    top_keywords = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_keywords = [feature_names[i] for i, score in top_keywords]\n",
    "    keywords.append(top_keywords)\n",
    "\n",
    "words_and_biz = zip(keywords, location_data)\n",
    "for words, biz in words_and_biz:\n",
    "    biz['biz_features'] = ', '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_output_object = {\n",
    "    \"businesses\": [\n",
    "        {\n",
    "            \"id\": \"0EWwmbkJ97nDZCPOdrWMau\",\n",
    "            \"category\": \"restaurant\",\n",
    "            \"business_summary\": \"A trendy and modern restaurant with a focus on local ingredients and sustainable practices.\",\n",
    "            \"personas\": [\n",
    "                \"ecoConsciousConsumer\",\n",
    "                \"culinaryExplorer\",\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"fWdKIXkM66qoLrllGfT7LR\",\n",
    "            \"category\": \"entertainment\",\n",
    "            \"business_summary\": \"A modern comedy club with a focus on local talent and a wide variety of shows.\",\n",
    "            \"personas\": [\n",
    "                \"socialButterfly\",\n",
    "                \"artCultureEnthusiast\",\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"Sg4N2msDQjYXuqmaR5a9OO\",\n",
    "            \"category\": \"activity\",\n",
    "            \"business_summary\": \"An envigorating and exciting hike through the Smoky Mountains.\",\n",
    "            \"personas\": [\n",
    "                \"adventurerExplorer\",\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = f\"\"\"\n",
    "You work at a travel agency. You are helping to process web scraped data from Yelp. Each webpage has already\n",
    "been preprocessed through an TF-IDF pipeline. Each business has been assigned a list of 50 keywords that\n",
    "describe the business.\n",
    "\n",
    "You will be provided with batches of 20 businesses.\n",
    "\n",
    "For each business providided with the Yelp ID, the name of the business, the categories of the business (as \n",
    "classified by Yelp), and the list of keywords produced by the TF-IDF pipeline. Your task is to use this \n",
    "information to produce a JSON object that describes the business. The JSON object you produce should contain \n",
    "the following information: \n",
    "- Category: The category of the business. You should pick one of the following categories: restaurant, activity, entertainment\n",
    "- Business Summary: A 1 sentence concise summary of the business that captures the essence and theme of the business.\n",
    "- Personas: The traveler personas that this business is a good fit for. You should pick 1-3 personas from the following list:\n",
    "    - 1. **The Social Butterfly**: A vibrant and outgoing individual who thrives in the energy of social gatherings, frequently found enjoying the nightlife at lively bars and clubs, and always up for a celebration with friends.\n",
    "    - 2. **The Culinary Explorer**: A gourmet aficionado who revels in culinary adventures, exploring diverse cuisines at fine dining establishments, and sharing their love for unique and delicious food experiences.\n",
    "    - 3. **The Beauty and Fashion Aficionado**: A trendsetter passionate about the latest in fashion and beauty, often seen at stylish shopping venues and beauty product launches, and always keeping up with the newest trends.\n",
    "    - 4. **The Family-Oriented Individual**: A person who cherishes family time and creates memories with loved ones, often participating in family-friendly activities, visiting parks, and enjoying experiences that cater to all ages.\n",
    "    - 5. **The Art and Culture Enthusiast**: A lover of the arts and culture, often found absorbing the rich experiences offered by museums and galleries, and always seeking to expand their horizons through artistic and cultural exploration.\n",
    "    - 6. **The Wellness and Self-Care Advocate**: A seeker of tranquility and personal well-being, often indulging in self-care routines, visiting wellness retreats and spas, and embracing serene natural environments for relaxation.\n",
    "    - 7. **The Adventurer and Explorer**: An intrepid soul with a thirst for adventure, often embarking on exciting journeys, exploring the great outdoors, and engaging in activities that offer a rush of adrenaline and connection with nature.\n",
    "    - 8. **The Eco-Conscious Consumer**: A dedicated advocate for sustainability and eco-friendly living, preferring to shop at environmentally conscious stores, visit farmers' markets, and support initiatives that align with their green lifestyle.\n",
    "\n",
    "\n",
    "As an example, here is an example of a valid JSON object that you should output:\n",
    "```\n",
    "{json.dumps(example_output_object, indent=4)}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "BUSINESS_PROMPT = \"\"\"\n",
    "ID: {id} | Name: {name}\n",
    "\n",
    "Yelp Categories: {categories}\n",
    "\n",
    "Buisness Keywords: {keywords}\n",
    "\"\"\"\n",
    "\n",
    "def format_batch(batch):\n",
    "    batch_str = \"\"\n",
    "    for biz in batch:\n",
    "        batch_str += BUSINESS_PROMPT.format(\n",
    "            id=biz['id'],\n",
    "            name=biz['name'],\n",
    "            categories=biz['categories'],\n",
    "            keywords=biz['biz_features']\n",
    "        ) + '\\n'\n",
    "    return batch_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken    \n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_batch(batch):\n",
    "    client = OpenAI()\n",
    "    human_prompt = format_batch(batch)\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "        {'role': 'user', 'content': human_prompt}\n",
    "    ]\n",
    "    completion = client.chat.completions.create(\n",
    "        model='gpt-3.5-turbo-1106',\n",
    "        messages=messages,\n",
    "        response_format={'type': 'json_object'}\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "completions = []\n",
    "for i in tqdm(range(0, len(location_data), 20)):\n",
    "    batch = location_data[i:i+20]\n",
    "    output = process_batch(batch)\n",
    "    try:\n",
    "        json_output = json.loads(output)\n",
    "        completions.extend(json_output['businesses'])\n",
    "    except KeyError as error:\n",
    "        print(f\"Error parsing output object: {error}\")\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for comp in completions:\n",
    "    for loc in location_data:\n",
    "        if loc['id'] == comp['id']:\n",
    "            loc['category'] = comp['category']\n",
    "            loc['business_summary'] = comp['business_summary']\n",
    "            loc['personas'] = comp['personas']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
