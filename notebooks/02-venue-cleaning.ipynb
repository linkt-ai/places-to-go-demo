{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Venue Data Cleaning\n",
    "\n",
    "Now that we have webscraped the reviews of our businesses, we need to clean the data so that it can be used to create Graph entites as well as documents for a pinecone index. The goal of this notebook is to populate two files:\n",
    "- `../data/cypher/venue-entities.json`: The data that will be used for writing the venue entites to Neo4J\n",
    "- `../data/vector/venue-documents.json`: The data that will be used for writing the venue documents to Pinecone\n",
    "\n",
    "## General Process\n",
    "\n",
    "**Data Loading**:\n",
    "We will start by reading out of the final file of the scraping process: `../data/scrape/locations_finished`. We will extract on the fields that we need to extract in order to create the embedding documents and the Venue entities.\n",
    "\n",
    "**Venue Embeddings**:\n",
    "To embed our venues, we will next format an embedding term for each venue, and use OpenAI's embedding model to create a venue embedding. This will allow us to get similarity scores for venues and natural language queries. \n",
    "\n",
    "**Cypher Entity Creation**:\n",
    "Next, we will use these embeddings to retrieve the most similar three keywords for each venue. These keywords will be related to the Venue in the graph, and they will help us determine how well a certain location is suited to a user.\n",
    "\n",
    "**Vectorstore Document Creation**:\n",
    "Finally, we will use the embeddings to create objects for embedding the venues into a pinecone index. We will want to keep the `category` and `city` fields to use as indexes for filter our results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Loading\n",
    "\n",
    "Let's start by loading our scraped location data, and extracting the necessary fields that we will need for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/scrape/locations_finished.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    location_data = json.load(f)\n",
    "    trimmed_location_data = []\n",
    "    for location in location_data:\n",
    "        trimmed_location_data.append({\n",
    "            \"id\": location['id'],\n",
    "            \"name\": location['name'],\n",
    "            \"city\": location['city_code'],\n",
    "            'rating': location['rating'],\n",
    "            \"reviews\": location['reviews'],\n",
    "        })\n",
    "    location_data = trimmed_location_data\n",
    "    del trimmed_location_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Venue Embeddings\n",
    "With the data loaded, we will now move on to embed each of our venues. To do this, we will create an embedding string that contains the name of the location, as well as the content of all of its scraped reviews. This should allow us to semantically search for venues, by comparing the semantic meaning of a user's query against the semantics of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add an embed term to each location\n",
    "for loc in location_data:\n",
    "    reviews = '\\n'.join([f\"{list(review.keys())[0]}:\\n{list(review.values())[0]}\" for review in loc['reviews']])\n",
    "    term = f\"{loc['name']}\\n\\n{reviews}\"\n",
    "    loc['embed_term'] = term\n",
    "\n",
    "embed_terms = [location['embed_term'] for location in location_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "def _embed_terms(terms: List[str]) -> List[List[float]]:\n",
    "    client = OpenAI()\n",
    "    response = client.embeddings.create(input=terms, model=\"text-embedding-ada-002\")\n",
    "    return [datum.embedding for datum in response.data]\n",
    "\n",
    "def embed_locations(embedding_terms: List[str]) -> List[List[float]]:\n",
    "    results = []\n",
    "    for i in range(0, len(embedding_terms), 2000):\n",
    "        batch = embedding_terms[i:i+2000]\n",
    "        results.extend(_embed_terms(batch))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"../data/location_embeddings.json\", \"r\") as f:\n",
    "        location_embeddings = json.load(f)\n",
    "        assert len(location_embeddings) == len(location_data) == len(embed_terms)\n",
    "except Exception:\n",
    "    location_embeddings = embed_locations(embed_terms)\n",
    "    assert len(location_embeddings) == len(location_data) == len(embed_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorstores.categories import CategoryVectorstore, CATEGORIES\n",
    "\n",
    "category_vectorstore = CategoryVectorstore()\n",
    "categories = category_vectorstore.search_categories(location_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorstores.personas import PersonaVectorstore\n",
    "\n",
    "persona_vectorstore = PersonaVectorstore()\n",
    "relevant_personas = persona_vectorstore.search_keywords(location_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cypher Entity Creation\n",
    "\n",
    "Now that we have our our category and keyword associations for each location, we will loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_and_data = zip(relevant_personas, categories, location_data)\n",
    "\n",
    "cypher_entities = []\n",
    "for i, data in enumerate(location_and_data):\n",
    "    personas, category, location = data\n",
    "    cypher_entities.append({\n",
    "        'venue': {\n",
    "            'id': location['id'],\n",
    "            'name': location['name'],\n",
    "            'city': location['city'],\n",
    "            'category': category,\n",
    "            'rating': location['rating'],\n",
    "        },\n",
    "        'personas': personas,\n",
    "    })\n",
    "assert len(cypher_entities) == len(location_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/venues/cypher_entities.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cypher_entities, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorstore Document Creation\n",
    "\n",
    "Finally, we want to create objects to use for writing our vector store documents to the pinecone instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_and_data = zip(location_data, location_embeddings, categories)\n",
    "\n",
    "vector_documents = []\n",
    "for i, data, in enumerate(location_and_data):\n",
    "    location, embedding, category = data\n",
    "    vector_documents.append({\n",
    "        \"id\": location['id'],\n",
    "        \"values\": embedding,\n",
    "        \"metadata\": {\n",
    "            \"category\": category,\n",
    "            \"city\": location['city']\n",
    "        }\n",
    "    })\n",
    "\n",
    "assert len(vector_documents) == len(location_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/venues/vectorstore_docs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vector_documents, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
