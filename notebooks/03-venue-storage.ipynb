{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage of Venue Data\n",
    "Now that we have extracted the keywords from our venue list, it is time to store the Venue's and keywords in a graph database. Additionally, we will need to store each venue in our document vectorstroe as well. To do this we will need to parse the extracted keyword-venue JSON objects to create Cypher Statements for writing the entities and relationships to our Neo4J database and then use the vectorstore docs to upsert our venue document vectors to Pinecone.\n",
    "\n",
    "This notebook will walk us through a few principle steps:\n",
    "1. **Creating Cyphers from the extracted JSON data**\n",
    "2. **Using Cyphers to write to Neo4J**\n",
    "3. **Writing venue documents to pinecone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "with open(\"../data/cypher_entities.json\", 'r') as location_data:\n",
    "    locations = json.load(location_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Cypher Statements from the extracted Venue data\n",
    "\n",
    "Now that we have our `cypher_entities`, we can use them to format our cypher statments, for writing a query to the graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union\n",
    "\n",
    "def make_safe(val: Union[str| float]) -> Union[str | float]:\n",
    "    \"\"\"Make a string with ' characters safe for Cypher\"\"\"\n",
    "    if type(val) == str:\n",
    "        return val.replace(\"'\", \"\\\\'\")\n",
    "    return val\n",
    "    \n",
    "def generate_cypher(venue: Dict[str, Union[str, float, List[str]]]) -> str:\n",
    "    e_statements = []\n",
    "    r_statements = []\n",
    "\n",
    "    venue_properties = \", \".join([f\"{key}: '{make_safe(value)}'\" for key, value in venue['venue'].items()]) if 'venue' in venue else \"\"\n",
    "    venue_cypher = f\"MERGE (v:Venue {{ {venue_properties} }})\"\n",
    "    e_statements.append(venue_cypher)\n",
    "\n",
    "    for i, keyword_data in enumerate(venue['keywords']):\n",
    "        keyword, weight = keyword_data\n",
    "        keyword_cypher = f\"MERGE (k{i+1}:Keyword {{ value: '{make_safe(keyword)}' }})\"\n",
    "        e_statements.append(keyword_cypher)\n",
    "        r_statements.append(f\"MERGE (v)-[r{i+1}:HAS_KEYWORD]->(k{i+1}) SET r{i+1}.weight = {weight}\") \n",
    "    \n",
    "    return e_statements, r_statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cypher_statements = []\n",
    "\n",
    "# Conver the cypher entity data into cypher statements\n",
    "for venue in locations:\n",
    "    e_statements, r_statements = generate_cypher(venue)\n",
    "    cypher_statemnt = \"\\n\".join(e_statements + r_statements)\n",
    "    cypher_statements.append(cypher_statemnt)\n",
    "\n",
    "assert len(cypher_statements) == len(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/venues/cypher_statements.json\", \"w\") as f:\n",
    "    json.dump(cypher_statements, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Cypher Statements to Write to Neo4J\n",
    "\n",
    "With our Cypher statements in hand, we will update the Neo4J database to store our Venue and Keyword entites. The usage of the 'MERGE' keyword will prevent nodes from being duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "DB_USER = os.getenv(\"NEO4J_DATABASE_USERNAME\")\n",
    "DB_URL = os.getenv(\"NEO4J_DATABASE_URL\")\n",
    "DB_PASSWORD = os.getenv(\"NEO4J_DATABASE_PASSWORD\")\n",
    "\n",
    "def get_driver():\n",
    "    return GraphDatabase.driver(DB_URL, auth=(DB_USER, DB_PASSWORD))\n",
    "\n",
    "driver = get_driver()\n",
    "\n",
    "with driver.session() as session:\n",
    "    for statement in cypher_statements:\n",
    "        session.run(statement)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the Venue Documents to Pinecone\n",
    "\n",
    "Next, we will load our vectorstore documents, and use them to write all of our documents to pinecone. Again, the use of the `upsert` method, will prevent records with the same ID from being duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\")\n",
    "PINECONE_INDEX = os.getenv(\"PINECONE_INDEX\")\n",
    "\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
    "index = pinecone.Index(PINECONE_INDEX)\n",
    "\n",
    "\n",
    "with open(\"../data/venues/vectorstore_docs.json\", \"r\") as f:\n",
    "    vectorstore_docs = json.load(f)\n",
    "\n",
    "assert len(vectorstore_docs) == len(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:21<00:00,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 2813 vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def store_docs(docs):\n",
    "    try:\n",
    "        BATCH_SIZE = 250\n",
    "        count = 0\n",
    "        for start in tqdm(range(0, len(docs), BATCH_SIZE)):\n",
    "            # Select the batch\n",
    "            batch = docs[start:start+BATCH_SIZE]\n",
    "            upsert_response = index.upsert(vectors=batch, namespace='venues')\n",
    "            \n",
    "            # Log the results\n",
    "            count += upsert_response['upserted_count']\n",
    "\n",
    "        print(f\"Upserted {count} vectors\")\n",
    "        return count\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0\n",
    "\n",
    "upserted_count = store_docs(vectorstore_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
