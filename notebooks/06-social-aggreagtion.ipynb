{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "DATA_DIR = \"../data/social\"\n",
        "\n",
        "# Add parent directory to path so we can import from 'vectorstores\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "def data_file(name: str) -> str: return f\"{DATA_DIR}/{name}.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Aggregation (TikTok)\n",
        "\n",
        "First we want to collect a set of TikTok posts to use for developing our social media post encoder. To do this, we will need to extract the `item_list` request that is used by the TikTok webapp to request video content during the infinite scroll on the For You Page. We can use inspect element to grab the URLs of these requests as we scroll on the For You Page. Then, by copying and pasting this URL into our python code, we can retrieve all the relevant metadata for the posts we are being fed by TikTok. \n",
        "\n",
        "This will provide us with plenty of video metadata, which we can then clean to only save the videos that have longer, more relevant metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "\n",
        "def extract_data(data: dict) -> dict:\n",
        "    extracted_data = {\n",
        "        \"id\": data['id'],\n",
        "        \"description\": data['desc'],\n",
        "        \"author\": {\n",
        "            \"id\": data['author']['id'],\n",
        "            \"name\": data['author']['nickname'],\n",
        "            \"signature\": data['author']['signature'],\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "def save_results(results) -> None:\n",
        "    data = results['itemList']\n",
        "\n",
        "    with open(data_file(\"tik_tok\"), \"r\", encoding=\"utf-8\") as tik_tok:\n",
        "        current_data = json.load(tik_tok)\n",
        "\n",
        "    with open(data_file(\"tik_tok\"), \"w\", encoding=\"utf-8\") as tik_tok:\n",
        "        extracted_data = [extract_data(item) for item in data]\n",
        "        current_data.extend(extracted_data)\n",
        "        json.dump(current_data, tik_tok, indent=4)\n",
        "        print(f\"Added {len(extracted_data)} new results to tik_tok.json. Total results: {len(current_data)}\")\n",
        "\n",
        "\n",
        "def pipeline() -> None:\n",
        "    url = input(\"Enter the url: \")\n",
        "    print(\"Requesting data...\")\n",
        "    results = requests.get(url)\n",
        "\n",
        "    if results.status_code == 200:\n",
        "        save_results(results.json())\n",
        "\n",
        "    else:\n",
        "        print(f\"Error: ({results.status_code}) {results.text})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total results: 108\n",
            "Unique results: 107\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Check for duplicates\n",
        "with open(data_file(\"tik_tok\"), \"r\", encoding=\"utf-8\") as tik_tok:\n",
        "    data = json.load(tik_tok)\n",
        "\n",
        "    ids = [item['id'] for item in data]\n",
        "    print(f\"Total results: {len(ids)}\")\n",
        "    print(f\"Unique results: {len(set(ids))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning\n",
        "Now that we have the data aggregated through our pipeline, we want to create a cleaning pipeline to remove all the entries with short description and limited metadata. Let's begin by doing some EDA on our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tiktoken\n",
        "\n",
        "def load_data() -> pd.DataFrame:\n",
        "    with open(data_file(\"tik_tok\"), \"r\", encoding=\"utf-8\") as tik_tok:\n",
        "        data = json.load(tik_tok)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "def annotate_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "    df['description_length'] = df['description'].apply(lambda val: len(enc.encode(val)))\n",
        "    df['author_signature_length'] = df['author_signature'].apply(lambda val: len(enc.encode(val)))\n",
        "\n",
        "    return df\n",
        "\n",
        "def overwrite_results(results) -> None:\n",
        "    with open(data_file(\"tik_tok\"), \"w\", encoding=\"utf-8\") as tik_tok:\n",
        "        json.dump(results, tik_tok, indent=4)\n",
        "        print(f\"Overwrote results in tik_tok.json. Total results: {len(results)}\")\n",
        "\n",
        "def clean_pipeline():\n",
        "    data = load_data()\n",
        "    df = annotate_data(data)\n",
        "\n",
        "    start = len(df)\n",
        "\n",
        "    # Filter out all rows with description length < 30\n",
        "    df = df[df['description_length'] > 30]\n",
        "    # Filter out all rows with author signature length < 20\n",
        "    df = df[df['author_signature_length'] > 20]\n",
        "\n",
        "    end = len(df)\n",
        "\n",
        "    print(f\"Filtered out {start - end} rows.\")\n",
        "\n",
        "    # Convert the dataframe back into a JSON object\n",
        "    data = json.loads(df.to_json(orient=\"records\"))\n",
        "    overwrite_results(data)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
